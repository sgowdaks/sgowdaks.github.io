<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Understanding Grouped Query Attention: The Secret Behind Efficient LLM Inference - Shivani Gowda KS</title>
  <link href="https://fonts.googleapis.com/css2?family=Source+Sans+3:wght@400;500;600;700&family=Source+Serif+4:wght@500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  
  <!-- Common Styles -->
  <link rel="stylesheet" href="../css/common.css">
  <link rel="stylesheet" href="../css/blog-post.css">
  
  <style>
    :root {
      --primary-color: #333333;
      --secondary-color: #555555;
      --accent-color: #666666;
      --text-color: #222222;
      --text-light: #666666;
      --bg-primary: #ffffff;
      --bg-secondary: #f8f9fa;
      --border-color: #dee2e6;
      --shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
      --gradient: linear-gradient(135deg, var(--primary-color), var(--secondary-color));
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Source Sans 3', 'Helvetica Neue', Arial, sans-serif;
      background: var(--bg-primary);
      color: var(--text-color);
      line-height: 1.7;
    }

    h1, h2, h3, h4, h5, h6 {
      font-family: 'Source Serif 4', 'Georgia', serif;
      font-weight: 600;
      color: var(--text-color);
    }

    .container {
      max-width: 800px;
      margin: 0 auto;
      padding: 2rem;
    }

    .post-header {
      text-align: center;
      margin-bottom: 3rem;
      padding-bottom: 2rem;
      border-bottom: 1px solid var(--border-color);
    }

    .post-title {
      font-size: 2.5rem;
      margin-bottom: 1rem;
      line-height: 1.2;
    }

    .post-subtitle {
      font-style: italic;
      color: var(--text-light);
      margin-bottom: 1.5rem;
      font-size: 1.1rem;
    }

    .post-meta {
      color: var(--text-light);
      font-size: 0.95rem;
      display: flex;
      justify-content: center;
      gap: 1.5rem;
      flex-wrap: wrap;
    }

    .post-category {
      background: var(--accent-color);
      color: white;
      padding: 6px 14px;
      border-radius: 15px;
      font-size: 0.8rem;
      font-weight: 600;
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }

    .post-content {
      font-size: 1.1rem;
      line-height: 1.8;
      margin-bottom: 3rem;
    }

    .post-content h2 {
      font-size: 1.8rem;
      margin: 2rem 0 1rem 0;
      color: var(--primary-color);
    }

    .post-content h3 {
      font-size: 1.4rem;
      margin: 1.5rem 0 0.8rem 0;
      color: var(--primary-color);
    }

    .post-content p {
      margin-bottom: 1.5rem;
    }

    .post-content ul, .post-content ol {
      margin: 1rem 0 1.5rem 2rem;
    }

    .post-content li {
      margin-bottom: 0.5rem;
    }

    .post-content blockquote {
      border-left: 4px solid var(--primary-color);
      padding-left: 1.5rem;
      margin: 1.5rem 0;
      font-style: italic;
      color: var(--text-light);
      background: var(--bg-secondary);
      padding: 1rem 1rem 1rem 2rem;
      border-radius: 4px;
    }

    .post-content code {
      background: var(--bg-secondary);
      padding: 0.2rem 0.4rem;
      border-radius: 4px;
      font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
      font-size: 0.9em;
      color: #d73a49;
    }

    .post-content pre {
      background: #f6f8fa;
      padding: 1.5rem;
      border-radius: 8px;
      overflow-x: auto;
      margin: 1.5rem 0;
      border: 1px solid var(--border-color);
      font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
      font-size: 0.85rem;
      line-height: 1.4;
    }

    .post-content pre code {
      background: none;
      padding: 0;
      color: #24292e;
    }

    .post-tags {
      display: flex;
      flex-wrap: wrap;
      gap: 0.5rem;
      margin-bottom: 2rem;
    }

    .tag {
      background: var(--bg-secondary);
      color: var(--text-color);
      padding: 6px 12px;
      border-radius: 15px;
      font-size: 0.8rem;
      border: 1px solid var(--border-color);
    }

    .back-to-blog {
      text-align: center;
      padding: 2rem 0;
      border-top: 1px solid var(--border-color);
    }

    .btn {
      display: inline-flex;
      align-items: center;
      gap: 0.5rem;
      padding: 12px 24px;
      background: var(--gradient);
      color: white;
      text-decoration: none;
      border-radius: 8px;
      font-weight: 500;
      transition: all 0.3s ease;
    }

    .btn:hover {
      transform: translateY(-2px);
      box-shadow: 0 8px 25px rgba(0, 0, 0, 0.2);
    }

    .highlight-box {
      background: #fff3cd;
      border: 1px solid #ffeaa7;
      border-left: 4px solid #f39c12;
      border-radius: 8px;
      padding: 1.5rem;
      margin: 1.5rem 0;
    }

    .highlight-box.danger {
      background: #f8d7da;
      border: 1px solid #f5c6cb;
      border-left: 4px solid #dc3545;
    }

    .highlight-box.success {
      background: #d4edda;
      border: 1px solid #c3e6cb;
      border-left: 4px solid #28a745;
    }

    .highlight-box p {
      margin-bottom: 0.5rem;
    }

    .highlight-box ul {
      margin-bottom: 0;
    }

    .post-content a {
      color: var(--primary-color);
      text-decoration: underline;
      transition: color 0.3s ease;
    }

    .post-content a:hover {
      color: var(--accent-color);
    }

    .table-container {
      overflow-x: auto;
      margin: 1.5rem 0;
    }

    .post-content table {
      width: 100%;
      border-collapse: collapse;
      margin: 1.5rem 0;
      font-size: 0.95rem;
    }

    .post-content table th,
    .post-content table td {
      border: 1px solid var(--border-color);
      padding: 12px;
      text-align: left;
    }

    .post-content table th {
      background: var(--bg-secondary);
      font-weight: 600;
      color: var(--primary-color);
    }

    .post-content table tbody tr:nth-child(even) {
      background: #f9f9f9;
    }

    .visual-box {
      background: var(--bg-secondary);
      border: 2px solid var(--border-color);
      border-radius: 8px;
      padding: 1.5rem;
      margin: 2rem 0;
      font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
      font-size: 0.9rem;
      line-height: 1.6;
    }

    @media (max-width: 768px) {
      .container {
        padding: 1rem;
      }
      
      .post-title {
        font-size: 2rem;
      }
      
      .post-meta {
        flex-direction: column;
        align-items: center;
        gap: 0.8rem;
      }
      
      .post-content {
        font-size: 1rem;
      }
      
      .post-content pre {
        font-size: 0.8rem;
        padding: 1rem;
      }
      
      .visual-box {
        font-size: 0.8rem;
        padding: 1rem;
      }
    }
  </style>
</head>

<body>
  <!-- Header (auto-generated by common.js) -->
  <header></header>

  <!-- Table of Contents Sidebar -->
  <nav class="toc-sidebar" id="toc-sidebar">
    <div class="toc-title">Table of Contents</div>
    <ul class="toc-list" id="toc-list">
      <!-- Auto-generated by script -->
    </ul>
  </nav>

  <div class="container">
    <div class="post-header">
      <h1 class="post-title">Understanding Grouped Query Attention: The Secret Behind Efficient LLM Inference</h1>
      <div class="post-subtitle">How reducing 32 KV heads to just 4 speeds up language models without breaking them</div>
      <div class="post-meta">
        <span class="post-category">Technology</span>
        <span><i class="far fa-calendar"></i> November 18, 2025</span>
        <span><i class="far fa-clock"></i> 15 min read</span>
      </div>
    </div>

    <div class="post-content">
      <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-bottom: 2rem;">[Disclaimer: This blog was written solely for my understanding purpose only. Any mistakes found that need to be addressed, please feel free to reach out to me.]</p>
      
      <blockquote>
        <p><strong>Note:</strong> In this post, we are focusing exclusively on Grouped Query Attention (GQA). While there are many other optimization techniques for LLM inference, this blog concentrates solely on understanding GQA and its memory efficiency benefits.</p>
      </blockquote>
      
      <h2>1. Why We Need GQA: The Memory Problem</h2>
      
      <p>When you chat with ChatGPT or generate text with any large language model (LLM), something interesting happens behind the scenes. The model doesn't just process your entire conversation from scratch every time it generates a new word. That would be incredibly slow!</p>

      <p>Instead, it uses a clever trick called <strong>KV caching</strong> (learn more in <a href="kv-caching-llm-inference-optimization.html">KV Caching: The Hidden Trick Behind Fast LLM Inference</a>). But even with caching, there's a problem: <strong>memory</strong>.</p>

      <h3>The Scale of the Problem: Qwen3-8B Example</h3>

      <p>In the <a href="kv-caching-llm-inference-optimization.html#total-ram-requirements">standard KV caching approach</a>, we calculated that Qwen3-8B requires the below memory requirements at different precisions:</p>

      <div class="table-container">
        <table>
          <thead>
            <tr>
              <th>Precision</th>
              <th>Bytes/Param</th>
              <th>Model Params</th>
              <th>KV Cache (Standard)</th>
              <th>Total RAM</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>FP32 (Float32)</strong></td>
              <td>4</td>
              <td>32.00 GB</td>
              <td>46.08 GB</td>
              <td><strong>81.12 GB</strong></td>
            </tr>
            <tr style="background: #fff3cd;">
              <td><strong>FP16 (Float16)</strong></td>
              <td>2</td>
              <td>16.00 GB</td>
              <td>23.04 GB</td>
              <td><strong>40.56 GB</strong></td>
            </tr>
            <tr>
              <td><strong>INT8 (8-bit)</strong></td>
              <td>1</td>
              <td>8.00 GB</td>
              <td>11.52 GB</td>
              <td><strong>20.28 GB</strong></td>
            </tr>
          </tbody>
        </table>
      </div>

      <p><strong>Even with FP16 (2 bytes), standard KV caching still requires 40.56 GB!</strong> This is where GQA becomes critical.</p>

      <h2>2. What is Grouped Query Attention (GQA)?</h2>

      <h3>A Quick Refresher: How Transformers Work</h3>

      <p>Transformer models (like GPT, Llama, Qwen) use something called <strong>attention</strong>. Think of it like this:</p>

      <p>When writing the next word, the model asks:</p>
      <ul>
        <li><strong>Query (Q):</strong> "What should I focus on?"</li>
        <li><strong>Key (K):</strong> "What information is available?"</li>
        <li><strong>Value (V):</strong> "What are the actual values of that information?"</li>
      </ul>

      <p>For each word, we compute:</p>
      <pre><code>Attention = softmax(q √ó k^T) √ó v</code></pre>

      <p>This tells the model which previous words to "pay attention to" when generating the next one.</p>

      <h3>Multi-Head Attention: Looking from Multiple Perspectives</h3>

      <p>Instead of doing this once, we do it <strong>32 times in parallel</strong> (in Qwen3-8B). Each is called a "head."</p>

      <p><strong>Why?</strong> Each head can learn to focus on different things:</p>
      <ul>
        <li>Head 1 might focus on grammar</li>
        <li>Head 2 might focus on topics</li>
        <li>Head 3 might focus on entities</li>
        <li>... and so on</li>
      </ul>

      <p>This is called <strong>Multi-Head Attention (MHA)</strong>.</p>

      <h2>The Traditional Approach: Multi-Head Attention (MHA)</h2>

      <p>In classic transformers:</p>
      <pre><code>32 Query heads (q)
32 Key heads (k)
32 Value heads (v)</code></pre>

      <p>Every q head gets its own k and v head. Simple and powerful!</p>

      <h3>The Memory Problem</h3>

      <p>Let's say you're generating a 1000-word response. For <strong>each layer</strong> (Qwen3-8B has 36 layers):</p>

      <p><strong>Memory needed for KV cache:</strong></p>
      <pre><code>32 k heads √ó 1000 tokens √ó 128 dimensions √ó 4 bytes = 16.4 MB
32 v heads √ó 1000 tokens √ó 128 dimensions √ó 4 bytes = 16.4 MB

Per layer: 32.8 MB
Total (36 layers): 1.18 GB just for the cache!</code></pre>

      <p>For longer conversations (10,000 tokens), you'd need <strong>11.8 GB</strong> just for KV cache!</p>

      <p><strong>Problem:</strong> This limits:</p>
      <ul>
        <li>How much you can fit on a GPU</li>
        <li>How many users you can serve simultaneously</li>
        <li>How long conversations can be</li>
      </ul>

      <h2>3. The Solution: Grouped Query Attention (GQA)</h2>

      <p>Here's the brilliant insight: <strong>Not every q head needs its own k and v!</strong></p>

      <p>Instead of 32 separate k/v heads, what if we used just <strong>8 k/v heads</strong> and <strong>shared</strong> them across query heads?</p>

      <blockquote>
        <p>From the Qwen3-8B configuration:</p>
        <pre><code>"num_attention_heads": 32,
"num_key_value_heads": 8,

groups = num_attention_heads / num_key_value_heads
       = 32 / 8 = 4

So each k/v head serves 4 query heads ‚Üí GQA</code></pre>
      </blockquote>

      <h3>Grouped Query Attention in Qwen3-8B</h3>

      <pre><code>32 Query heads (q)  ‚Üê‚îÄ‚îÄ‚îê
                       ‚îÇ
8 Key heads (k)        ‚îÇ Share!
8 Value heads (v)  ‚Üê‚îÄ‚îÄ‚îÄ‚îò

Each k/v head serves 4 q heads (32 √∑ 8 = 4)</code></pre>

      <p><strong>Grouping:</strong></p>
      <pre><code>q heads 0-3    ‚Üí Use k head 0, v head 0
q heads 4-7    ‚Üí Use k head 1, v head 1
q heads 8-11   ‚Üí Use k head 2, v head 2
q heads 12-15  ‚Üí Use k head 3, v head 3
q heads 16-19  ‚Üí Use k head 4, v head 4
q heads 20-23  ‚Üí Use k head 5, v head 5
q heads 24-27  ‚Üí Use k head 6, v head 6
q heads 28-31  ‚Üí Use k head 7, v head 7</code></pre>

      <h3>Memory Savings</h3>

      <p><strong>With GQA:</strong></p>
      <pre><code>8 K heads √ó 1000 tokens √ó 128 dimensions √ó 4 bytes = 4.1 MB
8 V heads √ó 1000 tokens √ó 128 dimensions √ó 4 bytes = 4.1 MB

Per layer: 8.2 MB (was 32.8 MB)
Total (36 layers): 295.2 MB (was 1.18 GB)

4√ó less memory! üéâ</code></pre>

      <h2>How Does This Actually Work? A Step-by-Step Dimensional Analysis</h2>

      <p>Let's trace through the dimensional transformations in GQA for Qwen3-8B processing 3 input tokens.</p>

      <h3>Input Dimensions</h3>

      <pre><code>Tokens: 3 (sequence_length)
Hidden size: 4096
Query heads: 32
K/V heads: 8 (GQA configuration)
Head dimension: 4096 / 32 = 128</code></pre>

      <h3>Step 1: Input Embedding</h3>

      <pre><code>Input tokens ‚Üí Token embeddings

Dimensions: [sequence_length, hidden_size]
           = [3, 4096]</code></pre>

      <h3>Step 2: Linear Projections to Q, K, V</h3>

      <p><strong>Query Projection (32 heads):</strong></p>
      <pre><code>q_proj: [3, 4096] ‚Üí [3, 32 √ó 128]
                  ‚Üí [3, 4096]

Reshape: [3, 4096] ‚Üí [3, 32, 128]
         [seq, hidden] ‚Üí [seq, num_heads, head_dim]</code></pre>

      <p><strong>Key Projection (8 heads - GQA!):</strong></p>
      <pre><code>k_proj: [3, 4096] ‚Üí [3, 8 √ó 128]
                  ‚Üí [3, 1024]

Reshape: [3, 1024] ‚Üí [3, 8, 128]
         [seq, hidden] ‚Üí [seq, num_kv_heads, head_dim]</code></pre>

      <p><strong>Value Projection (8 heads - GQA!):</strong></p>
      <pre><code>v_proj: [3, 4096] ‚Üí [3, 8 √ó 128]
                  ‚Üí [3, 1024]

Reshape: [3, 1024] ‚Üí [3, 8, 128]
         [seq, hidden] ‚Üí [seq, num_kv_heads, head_dim]</code></pre>

      <p><strong>Critical observation:</strong> K and V projections produce 4√ó fewer parameters than Q!</p>

      <h3>Step 3: Transpose for Attention Computation</h3>

      <pre><code>q: [3, 32, 128] ‚Üí [32, 3, 128]
   [seq, heads, head_dim] ‚Üí [heads, seq, head_dim]

k: [3, 8, 128] ‚Üí [8, 3, 128]
   [seq, kv_heads, head_dim] ‚Üí [kv_heads, seq, head_dim]

v: [3, 8, 128] ‚Üí [8, 3, 128]
   [seq, kv_heads, head_dim] ‚Üí [kv_heads, seq, head_dim]</code></pre>

      <h3>Step 4: Repeat k and v to Match q Heads (The GQA Magic!)</h3>

      <p>We have 8 k/v heads but need 32 to match q. Solution: repeat each k/v head 4 times (32 √∑ 8 = 4).</p>

      <pre><code># Original k and v dimensions
k: [8, 3, 128]   (8 unique k heads)
v: [8, 3, 128]   (8 unique v heads)

# Repeat to match q heads
k_expanded: [32, 3, 128]   (8 heads √ó 4 repetitions = 32)
v_expanded: [32, 3, 128]   (8 heads √ó 4 repetitions = 32)</code></pre>

      <p><strong>How the repetition works:</strong></p>
      <pre><code>Original k head 0 [3, 128] ‚Üí copied to positions 0, 1, 2, 3
Original k head 1 [3, 128] ‚Üí copied to positions 4, 5, 6, 7
Original k head 2 [3, 128] ‚Üí copied to positions 8, 9, 10, 11
Original k head 3 [3, 128] ‚Üí copied to positions 12, 13, 14, 15
Original k head 4 [3, 128] ‚Üí copied to positions 16, 17, 18, 19
Original k head 5 [3, 128] ‚Üí copied to positions 20, 21, 22, 23
Original k head 6 [3, 128] ‚Üí copied to positions 24, 25, 26, 27
Original k head 7 [3, 128] ‚Üí copied to positions 28, 29, 30, 31

Result: 32 k heads, but only 8 unique ones (each appears 4 times)</code></pre>

      <h3>Step 5: Attention Score Computation</h3>

      <pre><code># Now all dimensions align for matrix multiplication

q:           [32, 3, 128]
k^T:         [32, 128, 3]  (transposed last two dimensions)

Scores = q @ k^T

Dimensions:  [32, 3, 128] @ [32, 128, 3]
           = [32, 3, 3]
             [heads, seq_q, seq_k]

# Each head gets a 3√ó3 attention matrix
# (3 query positions attending to 3 key positions)</code></pre>

      <h3>Step 6: Apply Softmax and Multiply with v</h3>

      <pre><code>Attention_weights = softmax(Scores)
Dimensions: [32, 3, 3]

Output = Attention_weights @ v

Dimensions: [32, 3, 3] @ [32, 3, 128]
          = [32, 3, 128]
            [heads, seq, head_dim]</code></pre>

      <h3>Step 7: Concatenate Heads and Project</h3>

      <pre><code># Transpose back
[32, 3, 128] ‚Üí [3, 32, 128]

# Reshape (concatenate all heads)
[3, 32, 128] ‚Üí [3, 4096]

# Final output projection
o_proj: [3, 4096] ‚Üí [3, 4096]</code></pre>

      <h2>4. Memory Footprint Comparison</h2>

      <div class="table-container">
        <table>
          <thead>
            <tr>
              <th>Tensor</th>
              <th>Standard MHA Dimensions</th>
              <th>GQA Dimensions</th>
              <th>Memory Ratio</th>
            </tr>
          </thead>
          <tbody>     
            <tr style="background: #d4edda;">
              <td><strong>k (cached)</strong></td>
              <td>[<strong>32</strong>, 40960, 128]</td>
              <td>[<strong>8</strong>, 40960, 128]</td>
              <td><strong>4:1 (75% savings)</strong></td>
            </tr>
            <tr style="background: #d4edda;">
              <td><strong>v (cached)</strong></td>
              <td>[<strong>32</strong>, 40960, 128]</td>
              <td>[<strong>8</strong>, 40960, 128]</td>
              <td><strong>4:1 (75% savings)</strong></td>
            </tr>
          </tbody>
        </table>
      </div>

</code></pre>

      <h3>Combining GQA with Quantization</h3>

      <p>GQA becomes even more powerful when combined with quantization. Let's see the total RAM requirements:</p>

      <div class="table-container">
        <table>
          <thead>
            <tr>
              <th>Configuration</th>
              <th>Precision</th>
              <th>Model Params</th>
              <th>KV Cache</th>
              <th>Total RAM</th>
            </tr>
          </thead>
          <tbody>
            <tr style="background: #f8d7da;">
              <td>Standard (32 heads)</td>
              <td>FP32 (4 bytes)</td>
              <td>32.00 GB</td>
              <td>46.08 GB</td>
              <td><strong>81.12 GB</strong></td>
            </tr>
            <tr>
              <td>Standard (32 heads)</td>
              <td>FP16 (2 bytes)</td>
              <td>16.00 GB</td>
              <td>23.04 GB</td>
              <td><strong>40.56 GB</strong></td>
            </tr>
            <tr>
              <td>GQA (8 heads)</td>
              <td>FP32 (4 bytes)</td>
              <td>32.00 GB</td>
              <td>11.52 GB</td>
              <td>46.56 GB</td>
            </tr>
            <tr>
              <td>GQA (8 heads)</td>
              <td>FP16 (2 bytes)</td>
              <td>16.00 GB</td>
              <td>5.76 GB</td>
              <td>23.28 GB</td>
            </tr>
            <tr>
              <tr style="background: #d4edda;">
              <td><strong>GQA (8 heads)</strong></td>
              <td><strong>INT8 (1 byte)</strong></td>
              <td><strong>8.00 GB</strong></td>
              <td><strong>2.88 GB</strong></td>
              <td><strong>11.64 GB</strong></td>
            </tr>
          </tbody>
        </table>
      </div>
   
      <p>By grouping queries to share K/V heads, we get most of the benefits of multi-head attention with a fraction of the memory cost.</p>

      <p>This optimization is crucial for making large language models practical in production environments, where memory efficiency directly translates to cost savings and better user experience.</p>
    </div>

    <div class="post-footer-nav">
      <a href="azure-cognitive-services-network-monitoring.html" class="nav-btn nav-btn-prev">
        <i class="fas fa-arrow-left"></i>
        <div class="nav-btn-text">
          <span class="nav-label">Previous Post</span>
          <span class="nav-title">Getting Started with Azure Cognitive Services</span>
        </div>
      </a>
      <a href="kv-caching-llm-inference-optimization.html" class="nav-btn nav-btn-next">
        <div class="nav-btn-text">
          <span class="nav-label">Next Post</span>
          <span class="nav-title">KV Caching: The Hidden Engine Behind Fast LLM Inference</span>
        </div>
        <i class="fas fa-arrow-right"></i>
      </a>
    </div>

    <div class="post-tags">
      <span class="tag">LLM</span>
      <span class="tag">Machine Learning</span>
      <span class="tag">Attention</span>
      <span class="tag">Optimization</span>
      <span class="tag">AI</span>
    </div>

  </div>

  <!-- Footer (auto-generated by common.js) -->
  <footer></footer>

  <!-- Scripts -->
  <script src="../js/common.js"></script>
  
  <!-- Table of Contents Auto-Generator -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      const tocList = document.getElementById('toc-list');
      const postContent = document.querySelector('.post-content');
      const headings = postContent.querySelectorAll('h2');
      
      // Generate TOC items
      headings.forEach((heading, index) => {
        // Create ID for heading if it doesn't have one
        if (!heading.id) {
          heading.id = 'section-' + index;
        }
        
        // Create TOC list item
        const li = document.createElement('li');
        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;
        a.addEventListener('click', function(e) {
          e.preventDefault();
          heading.scrollIntoView({ behavior: 'smooth', block: 'start' });
          
          // Update active state
          document.querySelectorAll('.toc-list a').forEach(link => link.classList.remove('active'));
          a.classList.add('active');
        });
        
        li.appendChild(a);
        tocList.appendChild(li);
      });
      
      // Highlight active section on scroll
      window.addEventListener('scroll', function() {
        let current = '';
        
        headings.forEach(heading => {
          const sectionTop = heading.offsetTop;
          const scrollPos = window.scrollY + 150;
          
          if (scrollPos >= sectionTop) {
            current = heading.id;
          }
        });
        
        document.querySelectorAll('.toc-list a').forEach(a => {
          a.classList.remove('active');
          if (a.getAttribute('href') === '#' + current) {
            a.classList.add('active');
          }
        });
      });
    });
  </script>
</body>
</html>