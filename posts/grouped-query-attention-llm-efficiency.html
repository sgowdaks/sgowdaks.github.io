<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Understanding Grouped Query Attention: The Secret Behind Efficient LLM Inference - Shivani Gowda KS</title>
  <link href="https://fonts.googleapis.com/css2?family=Source+Sans+3:wght@400;500;600;700&family=Source+Serif+4:wght@500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  
  <!-- Common Styles -->
  <link rel="stylesheet" href="../css/common.css">
  <link rel="stylesheet" href="../css/blog-post.css">
  
  <style>
    :root {
      --primary-color: #333333;
      --secondary-color: #555555;
      --accent-color: #666666;
      --text-color: #222222;
      --text-light: #666666;
      --bg-primary: #ffffff;
      --bg-secondary: #f8f9fa;
      --border-color: #dee2e6;
      --shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
      --gradient: linear-gradient(135deg, var(--primary-color), var(--secondary-color));
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Source Sans 3', 'Helvetica Neue', Arial, sans-serif;
      background: var(--bg-primary);
      color: var(--text-color);
      line-height: 1.7;
    }

    h1, h2, h3, h4, h5, h6 {
      font-family: 'Source Serif 4', 'Georgia', serif;
      font-weight: 600;
      color: var(--text-color);
    }

    .container {
      max-width: 800px;
      margin: 0 auto;
      padding: 2rem;
    }

    .post-header {
      text-align: center;
      margin-bottom: 3rem;
      padding-bottom: 2rem;
      border-bottom: 1px solid var(--border-color);
    }

    .post-title {
      font-size: 2.5rem;
      margin-bottom: 1rem;
      line-height: 1.2;
    }

    .post-subtitle {
      font-style: italic;
      color: var(--text-light);
      margin-bottom: 1.5rem;
      font-size: 1.1rem;
    }

    .post-meta {
      color: var(--text-light);
      font-size: 0.95rem;
      display: flex;
      justify-content: center;
      gap: 1.5rem;
      flex-wrap: wrap;
    }

    .post-category {
      background: var(--accent-color);
      color: white;
      padding: 6px 14px;
      border-radius: 15px;
      font-size: 0.8rem;
      font-weight: 600;
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }

    .post-content {
      font-size: 1.1rem;
      line-height: 1.8;
      margin-bottom: 3rem;
    }

    .post-content h2 {
      font-size: 1.8rem;
      margin: 2rem 0 1rem 0;
      color: var(--primary-color);
    }

    .post-content h3 {
      font-size: 1.4rem;
      margin: 1.5rem 0 0.8rem 0;
      color: var(--primary-color);
    }

    .post-content p {
      margin-bottom: 1.5rem;
    }

    .post-content ul, .post-content ol {
      margin: 1rem 0 1.5rem 2rem;
    }

    .post-content li {
      margin-bottom: 0.5rem;
    }

    .post-content blockquote {
      border-left: 4px solid var(--primary-color);
      padding-left: 1.5rem;
      margin: 1.5rem 0;
      font-style: italic;
      color: var(--text-light);
      background: var(--bg-secondary);
      padding: 1rem 1rem 1rem 2rem;
      border-radius: 4px;
    }

    .post-content code {
      background: var(--bg-secondary);
      padding: 0.2rem 0.4rem;
      border-radius: 4px;
      font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
      font-size: 0.9em;
      color: #d73a49;
    }

    .post-content pre {
      background: #f6f8fa;
      padding: 1.5rem;
      border-radius: 8px;
      overflow-x: auto;
      margin: 1.5rem 0;
      border: 1px solid var(--border-color);
      font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
      font-size: 0.85rem;
      line-height: 1.4;
    }

    .post-content pre code {
      background: none;
      padding: 0;
      color: #24292e;
    }

    .post-tags {
      display: flex;
      flex-wrap: wrap;
      gap: 0.5rem;
      margin-bottom: 2rem;
    }

    .tag {
      background: var(--bg-secondary);
      color: var(--text-color);
      padding: 6px 12px;
      border-radius: 15px;
      font-size: 0.8rem;
      border: 1px solid var(--border-color);
    }

    .back-to-blog {
      text-align: center;
      padding: 2rem 0;
      border-top: 1px solid var(--border-color);
    }

    .btn {
      display: inline-flex;
      align-items: center;
      gap: 0.5rem;
      padding: 12px 24px;
      background: var(--gradient);
      color: white;
      text-decoration: none;
      border-radius: 8px;
      font-weight: 500;
      transition: all 0.3s ease;
    }

    .btn:hover {
      transform: translateY(-2px);
      box-shadow: 0 8px 25px rgba(0, 0, 0, 0.2);
    }

    .highlight-box {
      background: #fff3cd;
      border: 1px solid #ffeaa7;
      border-left: 4px solid #f39c12;
      border-radius: 8px;
      padding: 1.5rem;
      margin: 1.5rem 0;
    }

    .highlight-box.danger {
      background: #f8d7da;
      border: 1px solid #f5c6cb;
      border-left: 4px solid #dc3545;
    }

    .highlight-box.success {
      background: #d4edda;
      border: 1px solid #c3e6cb;
      border-left: 4px solid #28a745;
    }

    .highlight-box p {
      margin-bottom: 0.5rem;
    }

    .highlight-box ul {
      margin-bottom: 0;
    }

    .post-content a {
      color: var(--primary-color);
      text-decoration: underline;
      transition: color 0.3s ease;
    }

    .post-content a:hover {
      color: var(--accent-color);
    }

    .table-container {
      overflow-x: auto;
      margin: 1.5rem 0;
    }

    .post-content table {
      width: 100%;
      border-collapse: collapse;
      margin: 1.5rem 0;
      font-size: 0.95rem;
    }

    .post-content table th,
    .post-content table td {
      border: 1px solid var(--border-color);
      padding: 12px;
      text-align: left;
    }

    .post-content table th {
      background: var(--bg-secondary);
      font-weight: 600;
      color: var(--primary-color);
    }

    .post-content table tbody tr:nth-child(even) {
      background: #f9f9f9;
    }

    .visual-box {
      background: var(--bg-secondary);
      border: 2px solid var(--border-color);
      border-radius: 8px;
      padding: 1.5rem;
      margin: 2rem 0;
      font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
      font-size: 0.9rem;
      line-height: 1.6;
    }

    @media (max-width: 768px) {
      .container {
        padding: 1rem;
      }
      
      .post-title {
        font-size: 2rem;
      }
      
      .post-meta {
        flex-direction: column;
        align-items: center;
        gap: 0.8rem;
      }
      
      .post-content {
        font-size: 1rem;
      }
      
      .post-content pre {
        font-size: 0.8rem;
        padding: 1rem;
      }
      
      .visual-box {
        font-size: 0.8rem;
        padding: 1rem;
      }
    }
  </style>
</head>

<body>
  <!-- Header (auto-generated by common.js) -->
  <header></header>

  <div class="container">
    <div class="post-header">
      <h1 class="post-title">Understanding Grouped Query Attention: The Secret Behind Efficient LLM Inference</h1>
      <div class="post-subtitle">How reducing 32 KV heads to just 4 speeds up language models without breaking them</div>
      <div class="post-meta">
        <span class="post-category">Technology</span>
        <span><i class="far fa-calendar"></i> November 17, 2025</span>
        <span><i class="far fa-clock"></i> 15 min read</span>
      </div>
    </div>

    <div class="post-content">
      <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-bottom: 2rem;">[Disclaimer: This blog was written solely for my understanding purpose only. Any mistakes found that need to be addressed, please feel free to reach out to me.]</p>
      
      <blockquote>
        <p><strong>Note:</strong> In this post, we are focusing exclusively on Grouped Query Attention (GQA). While there are many other optimization techniques for LLM inference, this blog concentrates solely on understanding GQA and its memory efficiency benefits.</p>
      </blockquote>
      
      <h2>1. Why We Need GQA: The Memory Problem</h2>
      
      <p>When you chat with ChatGPT or generate text with any large language model (LLM), something interesting happens behind the scenes. The model doesn't just process your entire conversation from scratch every time it generates a new word. That would be incredibly slow!</p>

      <p>Instead, it uses a clever trick called <strong>KV caching</strong> (learn more in <a href="kv-caching-llm-inference-optimization.html">KV Caching: The Hidden Engine Behind Fast LLM Inference</a>). But even with caching, there's a problem: <strong>memory</strong>.</p>

      <h3>The Scale of the Problem: Qwen3-8B Example</h3>

      <p>In the <a href="kv-caching-llm-inference-optimization.html#total-ram-requirements">standard KV caching approach</a>, we calculated that Qwen3-8B requires:</p>

      <div class="highlight-box danger">
        <p><strong>Total RAM with Standard KV Caching (32 K/V heads):</strong></p>
        <ul>
          <li>Model Parameters: 32.00 GB</li>
          <li>KV Cache (40,960 tokens): <strong>46.08 GB</strong></li>
          <li>Other components: 3.04 GB</li>
          <li><strong>Total: 81.12 GB</strong></li>
        </ul>
        <p><strong>Problem:</strong> The KV cache alone consumes 56.8% of total memory!</p>
      </div>

      <p>This creates serious limitations:</p>
      <ul>
        <li><strong>Hardware Requirements:</strong> Need 80GB+ GPUs (expensive!)</li>
        <li><strong>Limited Throughput:</strong> Can only serve a few users simultaneously</li>
        <li><strong>Context Length:</strong> Can't handle very long conversations</li>
        <li><strong>Cost:</strong> Higher infrastructure costs for deployment</li>
      </ul>

      <p>The question becomes: <strong>Can we reduce this KV cache memory without sacrificing too much quality?</strong> The answer is Grouped Query Attention.</p>

      <h2>2. What is Grouped Query Attention (GQA)?</h2>

      <h3>A Quick Refresher: How Transformers Work</h3>

      <p>Transformer models (like GPT, Llama, Qwen) use something called <strong>attention</strong>. Think of it like this:</p>

      <p>When writing the next word, the model asks:</p>
      <ul>
        <li><strong>Query (Q):</strong> "What should I focus on?"</li>
        <li><strong>Key (K):</strong> "What information is available?"</li>
        <li><strong>Value (V):</strong> "What are the actual values of that information?"</li>
      </ul>

      <p>For each word, we compute:</p>
      <pre><code>Attention = softmax(Q Ã— K^T) Ã— V</code></pre>

      <p>This tells the model which previous words to "pay attention to" when generating the next one.</p>

      <h3>Multi-Head Attention: Looking from Multiple Perspectives</h3>

      <p>Instead of doing this once, we do it <strong>32 times in parallel</strong> (in Qwen3-8B). Each is called a "head."</p>

      <p><strong>Why?</strong> Each head can learn to focus on different things:</p>
      <ul>
        <li>Head 1 might focus on grammar</li>
        <li>Head 2 might focus on topics</li>
        <li>Head 3 might focus on entities</li>
        <li>... and so on</li>
      </ul>

      <p>This is called <strong>Multi-Head Attention (MHA)</strong>.</p>

      <h2>The Traditional Approach: Multi-Head Attention (MHA)</h2>

      <p>In classic transformers:</p>
      <pre><code>32 Query heads (Q)
32 Key heads (K)
32 Value heads (V)</code></pre>

      <p>Every Q head gets its own K and V head. Simple and powerful!</p>

      <h3>The Memory Problem</h3>

      <p>Let's say you're generating a 1000-word response. For <strong>each layer</strong> (Qwen3-8B has 36 layers):</p>

      <p><strong>Memory needed for KV cache:</strong></p>
      <pre><code>32 K heads Ã— 1000 tokens Ã— 128 dimensions Ã— 4 bytes = 16.4 MB
32 V heads Ã— 1000 tokens Ã— 128 dimensions Ã— 4 bytes = 16.4 MB

Per layer: 32.8 MB
Total (36 layers): 1.18 GB just for the cache!</code></pre>

      <p>For longer conversations (10,000 tokens), you'd need <strong>11.8 GB</strong> just for KV cache!</p>

      <p><strong>Problem:</strong> This limits:</p>
      <ul>
        <li>How much you can fit on a GPU</li>
        <li>How many users you can serve simultaneously</li>
        <li>How long conversations can be</li>
      </ul>

      <h2>The Solution: Grouped Query Attention (GQA)</h2>

      <p>Here's the brilliant insight: <strong>Not every Q head needs its own K and V!</strong></p>

      <p>Instead of 32 separate K/V heads, what if we used just <strong>8 K/V heads</strong> and <strong>shared</strong> them across query heads?</p>

      <blockquote>
        <p>From the Qwen3-8B configuration:</p>
        <pre><code>"num_attention_heads": 32,
"num_key_value_heads": 8,

groups = num_attention_heads / num_key_value_heads
       = 32 / 8 = 4

So each K/V head serves 4 query heads â†’ GQA</code></pre>
      </blockquote>

      <h3>Grouped Query Attention in Qwen3-8B</h3>

      <pre><code>32 Query heads (Q)  â†â”€â”€â”
                       â”‚
8 Key heads (K)        â”‚ Share!
8 Value heads (V)  â†â”€â”€â”€â”˜

Each K/V head serves 4 Q heads (32 Ã· 8 = 4)</code></pre>

      <p><strong>Grouping:</strong></p>
      <pre><code>Q heads 0-3    â†’ Use K head 0, V head 0
Q heads 4-7    â†’ Use K head 1, V head 1
Q heads 8-11   â†’ Use K head 2, V head 2
Q heads 12-15  â†’ Use K head 3, V head 3
Q heads 16-19  â†’ Use K head 4, V head 4
Q heads 20-23  â†’ Use K head 5, V head 5
Q heads 24-27  â†’ Use K head 6, V head 6
Q heads 28-31  â†’ Use K head 7, V head 7</code></pre>

      <h3>Memory Savings</h3>

      <p><strong>With GQA:</strong></p>
      <pre><code>8 K heads Ã— 1000 tokens Ã— 128 dimensions Ã— 4 bytes = 4.1 MB
8 V heads Ã— 1000 tokens Ã— 128 dimensions Ã— 4 bytes = 4.1 MB

Per layer: 8.2 MB (was 32.8 MB)
Total (36 layers): 295.2 MB (was 1.18 GB)

4Ã— less memory! ğŸ‰</code></pre>

      <h2>How Does This Actually Work? A Step-by-Step Dimensional Analysis</h2>

      <p>Let's trace through the dimensional transformations in GQA for Qwen3-8B processing 3 input tokens.</p>

      <h3>Input Dimensions</h3>

      <pre><code>Tokens: 3 (sequence_length)
Hidden size: 4096
Query heads: 32
K/V heads: 8 (GQA configuration)
Head dimension: 4096 / 32 = 128</code></pre>

      <h3>Step 1: Input Embedding</h3>

      <pre><code>Input tokens â†’ Token embeddings

Dimensions: [sequence_length, hidden_size]
           = [3, 4096]</code></pre>

      <h3>Step 2: Linear Projections to Q, K, V</h3>

      <p><strong>Query Projection (32 heads):</strong></p>
      <pre><code>q_proj: [3, 4096] â†’ [3, 32 Ã— 128]
                  â†’ [3, 4096]

Reshape: [3, 4096] â†’ [3, 32, 128]
         [seq, hidden] â†’ [seq, num_heads, head_dim]</code></pre>

      <p><strong>Key Projection (8 heads - GQA!):</strong></p>
      <pre><code>k_proj: [3, 4096] â†’ [3, 8 Ã— 128]
                  â†’ [3, 1024]

Reshape: [3, 1024] â†’ [3, 8, 128]
         [seq, hidden] â†’ [seq, num_kv_heads, head_dim]</code></pre>

      <p><strong>Value Projection (8 heads - GQA!):</strong></p>
      <pre><code>v_proj: [3, 4096] â†’ [3, 8 Ã— 128]
                  â†’ [3, 1024]

Reshape: [3, 1024] â†’ [3, 8, 128]
         [seq, hidden] â†’ [seq, num_kv_heads, head_dim]</code></pre>

      <p><strong>Critical observation:</strong> K and V projections produce 4Ã— fewer parameters than Q!</p>

      <h3>Step 3: Transpose for Attention Computation</h3>

      <pre><code>Q: [3, 32, 128] â†’ [32, 3, 128]
   [seq, heads, head_dim] â†’ [heads, seq, head_dim]

K: [3, 8, 128] â†’ [8, 3, 128]
   [seq, kv_heads, head_dim] â†’ [kv_heads, seq, head_dim]

V: [3, 8, 128] â†’ [8, 3, 128]
   [seq, kv_heads, head_dim] â†’ [kv_heads, seq, head_dim]</code></pre>

      <h3>Step 4: Repeat K and V to Match Q Heads (The GQA Magic!)</h3>

      <p>We have 8 K/V heads but need 32 to match Q. Solution: repeat each K/V head 4 times (32 Ã· 8 = 4).</p>

      <pre><code># Original K and V dimensions
K: [8, 3, 128]   (8 unique K heads)
V: [8, 3, 128]   (8 unique V heads)

# Repeat to match Q heads
K_expanded: [32, 3, 128]   (8 heads Ã— 4 repetitions = 32)
V_expanded: [32, 3, 128]   (8 heads Ã— 4 repetitions = 32)</code></pre>

      <p><strong>How the repetition works:</strong></p>
      <pre><code>Original K head 0 [3, 128] â†’ copied to positions 0, 1, 2, 3
Original K head 1 [3, 128] â†’ copied to positions 4, 5, 6, 7
Original K head 2 [3, 128] â†’ copied to positions 8, 9, 10, 11
Original K head 3 [3, 128] â†’ copied to positions 12, 13, 14, 15
Original K head 4 [3, 128] â†’ copied to positions 16, 17, 18, 19
Original K head 5 [3, 128] â†’ copied to positions 20, 21, 22, 23
Original K head 6 [3, 128] â†’ copied to positions 24, 25, 26, 27
Original K head 7 [3, 128] â†’ copied to positions 28, 29, 30, 31

Result: 32 K heads, but only 8 unique ones (each appears 4 times)</code></pre>

      <h3>Step 5: Attention Score Computation</h3>

      <pre><code># Now all dimensions align for matrix multiplication

Q:           [32, 3, 128]
K^T:         [32, 128, 3]  (transposed last two dimensions)

Scores = Q @ K^T

Dimensions:  [32, 3, 128] @ [32, 128, 3]
           = [32, 3, 3]
             [heads, seq_q, seq_k]

# Each head gets a 3Ã—3 attention matrix
# (3 query positions attending to 3 key positions)</code></pre>

      <h3>Step 6: Apply Softmax and Multiply with V</h3>

      <pre><code>Attention_weights = softmax(Scores)
Dimensions: [32, 3, 3]

Output = Attention_weights @ V

Dimensions: [32, 3, 3] @ [32, 3, 128]
          = [32, 3, 128]
            [heads, seq, head_dim]</code></pre>

      <h3>Step 7: Concatenate Heads and Project</h3>

      <pre><code># Transpose back
[32, 3, 128] â†’ [3, 32, 128]

# Reshape (concatenate all heads)
[3, 32, 128] â†’ [3, 4096]

# Final output projection
o_proj: [3, 4096] â†’ [3, 4096]</code></pre>

      <h3>Memory Footprint Comparison</h3>

      <div class="table-container">
        <table>
          <thead>
            <tr>
              <th>Tensor</th>
              <th>Standard MHA Dimensions</th>
              <th>GQA Dimensions</th>
              <th>Memory Ratio</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Query (Q)</strong></td>
              <td>[3, 32, 128]</td>
              <td>[3, 32, 128]</td>
              <td>1:1 (same)</td>
            </tr>
            <tr>
              <td><strong>Key (K)</strong></td>
              <td>[3, <strong>32</strong>, 128]</td>
              <td>[3, <strong>8</strong>, 128]</td>
              <td><strong>4:1 (75% savings)</strong></td>
            </tr>
            <tr>
              <td><strong>Value (V)</strong></td>
              <td>[3, <strong>32</strong>, 128]</td>
              <td>[3, <strong>8</strong>, 128]</td>
              <td><strong>4:1 (75% savings)</strong></td>
            </tr>
            <tr style="background: #d4edda;">
              <td><strong>K (cached)</strong></td>
              <td>[<strong>32</strong>, 40960, 128]</td>
              <td>[<strong>8</strong>, 40960, 128]</td>
              <td><strong>4:1 (75% savings)</strong></td>
            </tr>
            <tr style="background: #d4edda;">
              <td><strong>V (cached)</strong></td>
              <td>[<strong>32</strong>, 40960, 128]</td>
              <td>[<strong>8</strong>, 40960, 128]</td>
              <td><strong>4:1 (75% savings)</strong></td>
            </tr>
          </tbody>
        </table>
      </div>

      <h3>Key Dimensional Insights</h3>

      <div class="highlight-box">
        <p><strong>1. Projection Efficiency:</strong></p>
        <ul>
          <li>Q projection: 4096 â†’ 4096 (32 heads Ã— 128 dim)</li>
          <li>K projection: 4096 â†’ 1024 (8 heads Ã— 128 dim) <strong>[4Ã— smaller weight matrix]</strong></li>
          <li>V projection: 4096 â†’ 1024 (8 heads Ã— 128 dim) <strong>[4Ã— smaller weight matrix]</strong></li>
        </ul>

        <p><strong>2. Cache Efficiency (40,960 token context):</strong></p>
        <ul>
          <li>MHA K cache: [32, 40960, 128] = 167,772,160 elements</li>
          <li>GQA K cache: [8, 40960, 128] = 41,943,040 elements <strong>[4Ã— fewer]</strong></li>
          <li>Same 4Ã— reduction for V cache</li>
        </ul>

        <p><strong>3. Computational Grouping:</strong></p>
        <ul>
          <li>Group size = 32 Q heads / 8 K/V heads = 4</li>
          <li>Each K/V head serves 4 query heads</li>
          <li>Expansion: [8, seq, 128] â†’ [32, seq, 128] via repetition</li>
        </ul>
      </div>

      <h2>Visualizing the Grouping</h2>

      <p>Here's what happens in the attention computation:</p>

      <pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ QUERY HEADS (32 total)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚  Q0  Q1  Q2  Q3  â† Group 1 (4 Q heads)            â”‚
â”‚       â””â”€â”€â”¬â”€â”€â”˜                                      â”‚
â”‚          â†“                                         â”‚
â”‚      K0, V0  â† One K/V head shared by group 1     â”‚
â”‚                                                    â”‚
â”‚  Q4  Q5  Q6  Q7  â† Group 2 (4 Q heads)            â”‚
â”‚       â””â”€â”€â”¬â”€â”€â”˜                                      â”‚
â”‚          â†“                                         â”‚
â”‚      K1, V1  â† One K/V head shared by group 2     â”‚
â”‚                                                    â”‚
â”‚  Q8  Q9  Q10 Q11 â† Group 3 (4 Q heads)            â”‚
â”‚       â””â”€â”€â”¬â”€â”€â”˜                                      â”‚
â”‚          â†“                                         â”‚
â”‚      K2, V2  â† One K/V head shared by group 3     â”‚
â”‚                                                    â”‚
â”‚       ...continuing for all 8 groups...           â”‚
â”‚                                                    â”‚
â”‚  Q28 Q29 Q30 Q31 â† Group 8 (4 Q heads)            â”‚
â”‚       â””â”€â”€â”¬â”€â”€â”˜                                      â”‚
â”‚          â†“                                         â”‚
â”‚      K7, V7  â† One K/V head shared by group 8     â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>

      <p><strong>Each Q head still operates independently!</strong> They just share K and V.</p>

      <p><strong>GQA is the sweet spot:</strong> Great memory savings with minimal quality loss!</p>

      <h2>2. RAM Requirements with GQA: Qwen3-8B Analysis</h2>

      <p>Now let's calculate the actual RAM savings when using GQA instead of standard KV caching for Qwen3-8B.</p>

      <div class="visual-box">
Qwen3-8B Configuration (GQA): <br>
- hidden_size: 4096 <br>
- max_position_embeddings: 40960 <br>
- num_attention_heads: 32 <br>
- <strong>num_key_value_heads: 8</strong> (GQA!) <br>
- num_hidden_layers: 36 <br>
- Head dimension: 4096 / 32 = 128 <br>
- Precision: 4 bytes per parameter (float32)
      </div>

      <h3>KV Cache Memory with GQA</h3>

      <pre><code># GQA: Only 8 K/V heads instead of 32
sequence_length = 40960
num_layers = 36
num_key_value_heads = 8  # Reduced from 32!
head_dimension = 128
bytes_per_element = 4

# Per layer KV cache
k_cache_per_layer = sequence_length * num_key_value_heads * head_dimension * bytes_per_element
v_cache_per_layer = sequence_length * num_key_value_heads * head_dimension * bytes_per_element

k_cache_per_layer = 40960 * 8 * 128 * 4 = 167,772,160 bytes = 160 MB
v_cache_per_layer = 40960 * 8 * 128 * 4 = 167,772,160 bytes = 160 MB

# Total per layer: K + V
cache_per_layer = k_cache_per_layer + v_cache_per_layer = 335,544,320 bytes = 320 MB

# Across all layers
total_kv_cache_gqa = cache_per_layer * num_layers
total_kv_cache_gqa = 320 MB * 36 = 11.52 GB

# Compare to standard: 46.08 GB
Memory Reduction: 46.08 GB â†’ 11.52 GB (4Ã— reduction!)</code></pre>

      <h3>Total RAM Comparison</h3>

      <div class="table-container">
        <table>
          <thead>
            <tr>
              <th>Component</th>
              <th>Standard KV Cache</th>
              <th>GQA (Qwen3-8B)</th>
              <th>Savings</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Model Parameters</td>
              <td>32.00 GB</td>
              <td>32.00 GB</td>
              <td>0 GB</td>
            </tr>
            <tr>
              <td><strong>KV Cache</strong></td>
              <td><strong>46.08 GB</strong></td>
              <td><strong>11.52 GB</strong></td>
              <td><strong>34.56 GB (75%)</strong></td>
            </tr>
            <tr>
              <td>Activations</td>
              <td>0.64 GB</td>
              <td>0.64 GB</td>
              <td>0 GB</td>
            </tr>
            <tr>
              <td>Intermediate Tensors</td>
              <td>2.00 GB</td>
              <td>2.00 GB</td>
              <td>0 GB</td>
            </tr>
            <tr>
              <td>Memory Overhead</td>
              <td>0.40 GB</td>
              <td>0.40 GB</td>
              <td>0 GB</td>
            </tr>
            <tr style="background: #d4edda; font-weight: 600;">
              <td><strong>Total RAM</strong></td>
              <td><strong>81.12 GB</strong></td>
              <td><strong>46.56 GB</strong></td>
              <td><strong>34.56 GB (43%)</strong></td>
            </tr>
          </tbody>
        </table>
      </div>

      <div class="highlight-box success">
        <p><strong>ğŸ‰ Real-World Impact of GQA:</strong></p>
        <ul>
          <li><strong>Memory Reduction:</strong> 81.12 GB â†’ 46.56 GB (43% total savings)</li>
          <li><strong>KV Cache Reduction:</strong> 46.08 GB â†’ 11.52 GB (75% cache savings)</li>
          <li><strong>Hardware Requirement:</strong> Can now run on 48GB GPU instead of 80GB!</li>
          <li><strong>Cost Savings:</strong> A100 40GB/80GB â†’ RTX A6000 48GB (much cheaper)</li>
          <li><strong>Throughput:</strong> Can serve 3-4Ã— more users on same hardware</li>
          <li><strong>Quality Loss:</strong> Only ~1-3% performance degradation</li>
        </ul>
      </div>

      <h3>Why This Matters</h3>

      <p><strong>Before GQA (Standard KV Caching):</strong></p>
      <ul>
        <li>Need expensive 80GB A100 GPUs</li>
        <li>Can serve ~2-3 users per GPU with full context</li>
        <li>High deployment costs</li>
      </ul>

      <p><strong>After GQA:</strong></p>
      <ul>
        <li>Can use 48GB RTX A6000 or similar (much cheaper)</li>
        <li>Can serve ~7-10 users per GPU with full contex</li>
        <li>Significantly lower infrastructure costs</li>
        <li>Can afford longer context windows</li>
      </ul>    

      <h2>Summary</h2>

      <p>By grouping queries to share K/V heads, we get most of the benefits of multi-head attention with a fraction of the memory cost.</p>

      <p>This optimization is crucial for making large language models practical in production environments, where memory efficiency directly translates to cost savings and better user experience.</p>
    </div>

    <div class="post-footer-nav">
      <a href="azure-cognitive-services-network-monitoring.html" class="nav-btn nav-btn-prev">
        <i class="fas fa-arrow-left"></i>
        <div class="nav-btn-text">
          <span class="nav-label">Previous Post</span>
          <span class="nav-title">Getting Started with Azure Cognitive Services</span>
        </div>
      </a>
      <a href="kv-caching-llm-inference-optimization.html" class="nav-btn nav-btn-next">
        <div class="nav-btn-text">
          <span class="nav-label">Next Post</span>
          <span class="nav-title">KV Caching: The Hidden Engine Behind Fast LLM Inference</span>
        </div>
        <i class="fas fa-arrow-right"></i>
      </a>
    </div>

    <div class="post-tags">
      <span class="tag">LLM</span>
      <span class="tag">Machine Learning</span>
      <span class="tag">Attention</span>
      <span class="tag">Optimization</span>
      <span class="tag">AI</span>
    </div>

  </div>

  <!-- Footer (auto-generated by common.js) -->
  <footer></footer>

  <!-- Scripts -->
  <script src="../js/common.js"></script>
</body>
</html>