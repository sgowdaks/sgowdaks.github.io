<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Building a High-Performance ONNX Inference Engine for Qwen LLMs - Shivani Gowda KS</title>
  <link href="https://fonts.googleapis.com/css2?family=Source+Sans+3:wght@400;500;600;700&family=Source+Serif+4:wght@500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  
  <!-- Common Styles -->
  <link rel="stylesheet" href="../css/common.css">
  <link rel="stylesheet" href="../css/blog-post.css">
  
  <style>
    :root {
      --primary-color: #333333;
      --secondary-color: #555555;
      --accent-color: #666666;
      --text-color: #222222;
      --text-light: #666666;
      --bg-primary: #ffffff;
      --bg-secondary: #f8f9fa;
      --border-color: #dee2e6;
      --shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
      --gradient: linear-gradient(135deg, var(--primary-color), var(--secondary-color));
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Source Sans 3', 'Helvetica Neue', Arial, sans-serif;
      background: var(--bg-primary);
      color: var(--text-color);
      line-height: 1.7;
    }

    h1, h2, h3, h4, h5, h6 {
      font-family: 'Source Serif 4', 'Georgia', serif;
      font-weight: 600;
      color: var(--text-color);
    }

    .container {
      max-width: 800px;
      margin: 0 auto;
      padding: 2rem;
    }

    .post-header {
      text-align: center;
      margin-bottom: 3rem;
      padding-bottom: 2rem;
      border-bottom: 1px solid var(--border-color);
    }

    .post-title {
      font-size: 2.5rem;
      margin-bottom: 1rem;
      line-height: 1.2;
    }

    .post-subtitle {
      font-style: italic;
      color: var(--text-light);
      margin-bottom: 1.5rem;
      font-size: 1.1rem;
    }

    .post-meta {
      color: var(--text-light);
      font-size: 0.95rem;
      display: flex;
      justify-content: center;
      gap: 1.5rem;
      flex-wrap: wrap;
    }

    .post-category {
      background: var(--accent-color);
      color: white;
      padding: 6px 14px;
      border-radius: 15px;
      font-size: 0.8rem;
      font-weight: 600;
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }

    .post-content {
      font-size: 1.1rem;
      line-height: 1.8;
      margin-bottom: 3rem;
    }

    .post-content h2 {
      font-size: 1.8rem;
      margin: 2rem 0 1rem 0;
      color: var(--primary-color);
    }

    .post-content h3 {
      font-size: 1.4rem;
      margin: 1.5rem 0 0.8rem 0;
      color: var(--primary-color);
    }

    .post-content p {
      margin-bottom: 1.5rem;
    }

    .post-content ul, .post-content ol {
      margin: 1rem 0 1.5rem 2rem;
    }

    .post-content li {
      margin-bottom: 0.5rem;
    }

    .post-content blockquote {
      border-left: 4px solid var(--primary-color);
      padding-left: 1.5rem;
      margin: 1.5rem 0;
      font-style: italic;
      color: var(--text-light);
      background: var(--bg-secondary);
      padding: 1rem 1rem 1rem 2rem;
      border-radius: 4px;
    }

    .post-content code {
      background: var(--bg-secondary);
      padding: 0.2rem 0.4rem;
      border-radius: 4px;
      font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
      font-size: 0.9em;
      color: #d73a49;
    }

    .post-content pre {
      background: #f6f8fa;
      padding: 1.5rem;
      border-radius: 8px;
      overflow-x: auto;
      margin: 1.5rem 0;
      border: 1px solid var(--border-color);
      font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
      font-size: 0.85rem;
      line-height: 1.4;
    }

    .post-content pre code {
      background: none;
      padding: 0;
      color: #24292e;
    }

    .post-tags {
      display: flex;
      flex-wrap: wrap;
      gap: 0.5rem;
      margin: 2rem 0;
      justify-content: center;
    }

    .tag {
      background: var(--bg-secondary);
      color: var(--text-color);
      padding: 6px 12px;
      border-radius: 15px;
      font-size: 0.8rem;
      border: 1px solid var(--border-color);
    }

    .back-to-blog {
      text-align: center;
      padding: 2rem 0;
      border-top: 1px solid var(--border-color);
    }

    .btn {
      display: inline-flex;
      align-items: center;
      gap: 0.5rem;
      padding: 12px 24px;
      background: var(--gradient);
      color: white;
      text-decoration: none;
      border-radius: 8px;
      font-weight: 500;
      transition: all 0.3s ease;
    }

    .btn:hover {
      transform: translateY(-2px);
      box-shadow: 0 8px 25px rgba(0, 0, 0, 0.2);
    }

    .post-content a {
      color: var(--primary-color);
      text-decoration: underline;
      transition: color 0.3s ease;
    }

    .post-content a:hover {
      color: var(--accent-color);
    }

    .table-container {
      overflow-x: auto;
      margin: 1.5rem 0;
    }

    .post-content table {
      width: 100%;
      border-collapse: collapse;
      margin: 1.5rem 0;
      font-size: 0.95rem;
    }

    .post-content table th,
    .post-content table td {
      border: 1px solid var(--border-color);
      padding: 12px;
      text-align: left;
    }

    .post-content table th {
      background: var(--bg-secondary);
      font-weight: 600;
      color: var(--primary-color);
    }

    .post-content table tbody tr:nth-child(even) {
      background: #f9f9f9;
    }

    .visual-box {
      background: var(--bg-secondary);
      border: 2px solid var(--border-color);
      border-radius: 8px;
      padding: 1.5rem;
      margin: 2rem 0;
      font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
      font-size: 0.9rem;
      line-height: 1.6;
    }

    .comparison-box {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 1rem;
      margin: 2rem 0;
    }

    .comparison-item {
      background: var(--bg-secondary);
      border: 1px solid var(--border-color);
      border-radius: 8px;
      padding: 1rem;
    }

    .comparison-item h4 {
      color: var(--primary-color);
      margin-bottom: 0.5rem;
      font-size: 1rem;
    }

    .highlight-box {
      background: #fff3cd;
      border: 1px solid #ffeaa7;
      border-radius: 8px;
      padding: 1rem;
      margin: 1.5rem 0;
    }

    .highlight-box.danger {
      background: #f8d7da;
      border-color: #f5c6cb;
    }

    .highlight-box.success {
      background: #d4edda;
      border-color: #c3e6cb;
    }

    @media (max-width: 768px) {
      .container {
        padding: 1rem;
      }
      
      .post-title {
        font-size: 2rem;
      }
      
      .post-meta {
        flex-direction: column;
        align-items: center;
        gap: 0.8rem;
      }
      
      .post-content {
        font-size: 1rem;
      }
      
      .post-content pre {
        font-size: 0.8rem;
        padding: 1rem;
      }
      
      .visual-box {
        font-size: 0.8rem;
        padding: 1rem;
      }

      .comparison-box {
        grid-template-columns: 1fr;
      }
    }
  </style>
</head>

<body>
  <!-- Header (auto-generated by common.js) -->
  <header></header>

  <!-- Table of Contents Sidebar -->
  <nav class="toc-sidebar" id="toc-sidebar">
    <div class="toc-title">Table of Contents</div>
    <ul class="toc-list" id="toc-list">
      <!-- Auto-generated by script -->
    </ul>
  </nav>

  <div class="container">
    <div class="post-header">
      <h1 class="post-title">Building a High-Performance ONNX Inference Engine for LLMs</h1>
      <div class="post-subtitle">From PyTorch to C++ with GPU Acceleration</div>
      <div class="post-meta">
        <span class="post-category">Technology</span>
        <span><i class="far fa-calendar"></i> November 20, 2025</span>
        <span><i class="far fa-clock"></i> 10 min read</span>
      </div>
    </div>

    <div class="post-content">
<!-- <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-bottom: 2rem;">[Disclaimer: This blog was written solely for my understanding purpose only. Any mistakes found that need to be addressed, please feel free to reach out to me.]</p> -->
 <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-bottom: 2rem;">[Disclaimer: This blog was written solely for my understanding purpose only. Any mistakes found that need to be addressed, please feel free to reach out to me]</p>
      <h1>Building a High-Performance ONNX Inference Engine for Qwen LLMs: From PyTorch to C++ with GPU Acceleration</h1>
<p><em>A deep dive into exporting Qwen language models to ONNX and building a production-ready C++ inference engine</em></p>

<h2>TL;DR</h2>
<p>I'm building a high-performance ONNX inference engine for Qwen language models in C++ with GPU acceleration. Currently achieving <strong>~16.84 tokens/sec on GPU</strong>, with significant optimization opportunities identified. This article covers: dealing with HuggingFace Transformers' dynamic cache limitations, implementing a custom forward pass, exporting to ONNX, fixing ONNX Runtime API issues, and GPU inference with cuDNN 9. Performance optimization is actively underway.</p>
<p><strong>GitHub Repository:</strong> <a href="https://github.com/sgowdaks/llm-inference">llm-inference</a></p>
<div class="highlight-box highlight-warning">
<p><strong>‚ö†Ô∏è Current Status:</strong> The C++ inference engine is functionally complete but performance is lower than expected (16.84 tokens/sec vs Python's 21.78 tokens/sec). I'm actively working on optimization. See "Performance Benchmarking" and "Next Steps" sections for details on where improvements will focus.</p>
</div>
<div class="highlight-box highlight-info">
<p><strong>Attribution:</strong> The custom forward pass implementation and ONNX export approach were adapted from <a href="https://github.com/DakeQQ/Native-LLM-for-Android" target="_blank">DakeQQ/Native-LLM-for-Android</a>, an excellent project for running LLMs on mobile devices. I've extended and modified these techniques for server-side GPU inference.</p>
</div>

<h2>The Challenge: Running LLMs Efficiently</h2>

<p>Large Language Models (LLMs) are powerful but resource-intensive. While Python frameworks like HuggingFace Transformers and PyTorch make it easy to prototype and train models, production inference deployments face significant challenges.</p>

<h3>The PyTorch Overhead Problem</h3>

<p>PyTorch is the go-to framework for training LLMs, and for good reasons - it's flexible, easy to debug, and has excellent GPU support. But here's what's happening under the hood:</p>

<div class="highlight-box highlight-warning">
<p><strong>PyTorch's Architecture:</strong></p>
<ul>
  <li><strong>Python frontend:</strong> Your code that defines models, training loops, etc.</li>
  <li><strong>pybind11 bindings:</strong> Translates Python calls to C++</li>
  <li><strong>libtorch (C++):</strong> The actual computation engine
    <ul>
      <li>ATen tensor library</li>
      <li>Autograd for gradients</li>
      <li>CUDA/cuDNN kernels for GPU acceleration</li>
    </ul>
  </li>
</ul>
<p><strong>Key insight:</strong> Even though PyTorch uses C++ and CUDA for the heavy lifting, <em>Python still orchestrates everything</em> - deciding which operations to run, when, and managing the model structure.</p>
</div>

<p>This Python orchestration layer introduces overhead:</p>
<ul>
  <li>üî¥ <strong>Interpreter:</strong> Python interprets code at runtime (no compilation)</li>
  <li>üî¥ <strong>GIL (Global Interpreter Lock):</strong> Limits true multi-threading</li>
  <li>üî¥ <strong>Dynamic graph construction:</strong> Model structure rebuilt for each forward pass</li>
  <li>üî¥ <strong>Memory overhead:</strong> Python runtime + garbage collector (~200-300MB)</li>
  <li>üî¥ <strong>Boundary crossings:</strong> Frequent Python ‚Üî C++ calls add latency</li>
</ul>

<h3>Training vs Inference: Different Requirements</h3>

<p>Here's a key insight that shapes modern ML workflows:</p>

<div class="comparison-box">
  <div class="comparison-item">
    <h4>üìö Training (Python is fine)</h4>
    <ul>
      <li><strong>Frequency:</strong> Done once or periodically</li>
      <li><strong>Priority:</strong> Flexibility and debuggability</li>
      <li><strong>Workflow:</strong> Iterative experimentation</li>
      <li><strong>Hardware:</strong> Usually on powerful GPU clusters</li>
      <li><strong>Python advantages:</strong>
        <ul>
          <li>Easy debugging (print statements, breakpoints)</li>
          <li>Rich ecosystem (data loaders, visualization)</li>
          <li>Quick iteration on model architectures</li>
          <li>Autograd makes gradient computation simple</li>
        </ul>
      </li>
    </ul>
    <p><strong>Verdict:</strong> Python overhead is acceptable - flexibility matters more</p>
  </div>

  <div class="comparison-item">
    <h4>‚ö° Inference (C++ shines)</h4>
    <ul>
      <li><strong>Frequency:</strong> Millions of times per day</li>
      <li><strong>Priority:</strong> Speed, cost, and efficiency</li>
      <li><strong>Workflow:</strong> Fixed model, just run it</li>
      <li><strong>Hardware:</strong> Cost-optimized, often edge devices</li>
      <li><strong>C++ advantages:</strong>
        <ul>
          <li>No interpreter overhead</li>
          <li>Lower memory footprint</li>
          <li>Faster startup time</li>
          <li>True multi-threading</li>
          <li>Deployable anywhere (mobile, IoT, servers)</li>
        </ul>
      </li>
    </ul>
    <p><strong>Verdict:</strong> Every millisecond counts - remove Python overhead</p>
  </div>
</div>

<div class="highlight-box highlight-success">
<p><strong>Common ML Production Pattern:</strong></p>
<ol>
  <li><strong>Train in Python/PyTorch:</strong> Leverage flexibility, debugging, rich ecosystem</li>
  <li><strong>Export to ONNX:</strong> Convert trained model to a portable, optimized format</li>
  <li><strong>Deploy with C++ ONNX Runtime:</strong> Run inference without Python overhead</li>
</ol>
<p>This gives you the best of both worlds: Python's ease for training, C++'s speed for inference.</p>
</div>

<h3>Why Training in C++ is Hard (and Rare)</h3>

<p>You might wonder: "If C++ is so fast, why not train in C++ too?"</p>

<p><strong>The reality:</strong> Training requires:</p>
<ul>
  <li>Frequent code changes (trying architectures, hyperparameters)</li>
  <li>Complex gradient computation (autograd)</li>
  <li>Rich debugging (inspecting tensors, gradients, losses)</li>
  <li>Data pipelines (augmentation, batching, sampling)</li>
  <li>Integration with visualization tools (TensorBoard, wandb)</li>
</ul>

<p>Implementing all of this in C++ is possible but <strong>extremely time-consuming</strong>. A research experiment that takes 1 day in Python might take 1-2 weeks in C++. The productivity cost outweighs the performance gain for training (which is done infrequently).</p>

<p><strong>For inference though</strong>, the model is frozen - no more experimentation. You just need to run the same computation graph millions of times. This is where C++'s performance advantage justifies the effort.</p>

<h3>What We Need for Production Inference</h3>

<p>To deploy LLMs efficiently, we need:</p>
<ul>
<li>‚úÖ <strong>Faster inference</strong> (lower latency, higher throughput)</li>
<li>‚úÖ <strong>Better resource utilization</strong> (more requests per GPU)</li>
<li>‚úÖ <strong>Lower deployment overhead</strong> (no Python interpreter, smaller containers)</li>
<li>‚úÖ <strong>Cross-platform compatibility</strong> (server, mobile, edge)</li>
<li>‚úÖ <strong>Static optimization</strong> (graph fusion, kernel selection)</li>
</ul>

<p><strong>The solution?</strong> ONNX (Open Neural Network Exchange) - a format that bridges training (Python) and inference (C++), providing a standardized, optimized way to deploy models across different runtimes and hardware.</p>

<p>But getting there isn't straightforward, especially for complex models like Qwen...</p>


<h2>What is ONNX?</h2>
<p>ONNX is an open, framework-agnostic format for representing neural networks as a computational graph. It standardizes how layers and operations are described so that models trained in one framework (e.g., PyTorch) can run efficiently across many runtimes and hardware backends.</p>

<h3>Dynamic vs Static Execution: A Graph Traversal Analogy</h3>
<p>To understand the fundamental difference between Python/PyTorch execution and ONNX, think of graph traversal algorithms like BFS (Breadth-First Search):</p>

<div class="comparison-box">
  <div class="comparison-item">
    <h4>üêç Python-style (Dynamic Execution)</h4>
    <div class="codehilite"><pre><span></span><code><span class="c1"># BFS graph built on the spot</span>
<span class="n">graph</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">2</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="mi">3</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">]}</span>
<span class="n">queue</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">while</span> <span class="n">queue</span><span class="p">:</span>
    <span class="n">node</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">visit</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>   <span class="c1"># do something</span>
</code></pre></div>
    <p><strong>Characteristics:</strong></p>
    <ul>
      <li>Graph and traversal happen <em>together</em></li>
      <li>Flexible, but every run rebuilds or interprets the structure</li>
      <li>Python interpreter evaluates each line dynamically</li>
    </ul>
  </div>

  <div class="comparison-item">
    <h4>üìä ONNX-style (Precompiled Graph)</h4>
    <div class="codehilite"><pre><span></span><code><span class="c1"># BFS graph &quot;prebuilt&quot; in a file</span>
<span class="c1"># Runtime only plugs in start node</span>
<span class="n">start_node</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">run_precompiled_graph</span><span class="p">(</span><span class="n">start_node</span><span class="p">)</span>
</code></pre></div>
    <p><strong>Characteristics:</strong></p>
    <ul>
      <li>Graph structure <em>fixed in advance</em></li>
      <li>Runtime just feeds input ‚Üí gets output</li>
      <li>Efficient, portable, no dynamic construction</li>
    </ul>
  </div>
</div>

<div class="highlight-box highlight-success">
<p><strong>Analogy:</strong></p>
<ul>
  <li><strong>Python/PyTorch</strong> = "Draw the graph while traversing" - flexible but slower</li>
  <li><strong>ONNX</strong> = "Graph is drawn once, later you just plug in inputs" - fast and portable</li>
</ul>
<p>This is why ONNX models run faster: the computational graph is frozen at export time, and the runtime can optimize execution without worrying about dynamic changes.</p>
</div>


<h2>ONNX + Python vs ONNX + C++: Why Both Matter</h2>

<p>Now that we understand ONNX is a file format and it acts like a blueprint or recipe:</p>
<ul>
  <li>üìÑ <strong>ONNX file (.onnx)</strong> = The "blueprint" - describes what operations to run and how they connect</li>
  <li>üè≠ <strong>ONNX Runtime</strong> = The "factory" - reads the blueprint and executes it</li>
  <li>üîß <strong>Programming language (Python/C++)</strong> = How you interact with the factory</li>
</ul>

<p>The ONNX file itself doesn't run anything - it's just a standardized description of the model. You need an <strong>ONNX Runtime</strong> to actually execute it, and you need to call that runtime from <em>some</em> programming language.</p>

<h3>Python vs C++ ONNX Runtime: The Critical Difference</h3>

<div class="comparison-box">
  <div class="comparison-item">
    <h4>üêç Python + ONNX Runtime</h4>
    <div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">onnxruntime</span> <span class="k">as</span> <span class="nn">ort</span>

<span class="c1"># Load ONNX model</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">ort</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="s2">&quot;model.onnx&quot;</span><span class="p">)</span>

<span class="c1"># Run inference</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="n">input_data</span><span class="p">})</span>
</code></pre></div>
    <p><strong>What happens:</strong></p>
    <ol>
      <li>Python interpreter starts</li>
      <li>Loads Python ONNX Runtime bindings (pybind11 wrapper)</li>
      <li>Python ‚Üí C++ boundary crossing for <em>every</em> API call</li>
      <li>Data conversion: numpy array ‚Üí C++ tensor ‚Üí GPU memory</li>
      <li>ONNX Runtime (C++) does the actual computation</li>
      <li>Results converted back: GPU ‚Üí C++ tensor ‚Üí numpy ‚Üí Python</li>
    </ol>
    <p><strong>Overhead sources:</strong></p>
    <ul>
      <li>Python interpreter startup (~200-500ms)</li>
      <li>GIL (Global Interpreter Lock) for thread safety</li>
      <li>Python ‚Üí C++ function call overhead</li>
      <li>Data type conversions and memory copies</li>
      <li>Python object management and garbage collection</li>
    </ul>
  </div>

  <div class="comparison-item">
    <h4>‚ö° C++ + ONNX Runtime</h4>
    <div class="codehilite"><pre><span></span><code><span class="cp">#include</span> <span class="cpf">&lt;onnxruntime_cxx_api.h&gt;</span>

<span class="c1">// Load ONNX model</span>
<span class="n">Ort</span><span class="o">::</span><span class="n">Session</span> <span class="n">session</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="s">&quot;model.onnx&quot;</span><span class="p">,</span> <span class="n">options</span><span class="p">);</span>

<span class="c1">// Run inference</span>
<span class="k">auto</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">session</span><span class="p">.</span><span class="n">Run</span><span class="p">(</span><span class="n">run_options</span><span class="p">,</span> 
                           <span class="n">input_names</span><span class="p">,</span> 
                           <span class="o">&amp;</span><span class="n">input_tensor</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                           <span class="n">output_names</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</code></pre></div>
    <p><strong>What happens:</strong></p>
    <ol>
      <li>Native C++ executable starts</li>
      <li>Directly calls ONNX Runtime C++ API (no wrapper)</li>
      <li><strong>Zero</strong> language boundary crossing</li>
      <li>Direct memory access: C++ ‚Üí GPU (no conversions)</li>
      <li>ONNX Runtime does computation</li>
      <li>Results stay in C++ memory (no conversions)</li>
    </ol>
    <p><strong>Advantages:</strong></p>
    <ul>
      <li>Fast startup (~50-100ms)</li>
      <li>No GIL, true multi-threading</li>
      <li>Direct function calls (no overhead)</li>
      <li>Zero-copy operations where possible</li>
      <li>Manual memory control, no GC pauses</li>
    </ul>
  </div>
</div>

<h3>The Performance Stack</h3>

<div class="highlight-box highlight-warning">
<p><strong>Understanding the layers:</strong></p>
<table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
  <thead>
    <tr style="background: var(--bg-secondary); border-bottom: 2px solid var(--border-color);">
      <th style="padding: 10px; text-align: left;">Layer</th>
      <th style="padding: 10px; text-align: center;">Python ONNX</th>
      <th style="padding: 10px; text-align: center;">C++ ONNX</th>
    </tr>
  </thead>
  <tbody>
    <tr style="border-bottom: 1px solid var(--border-color);">
      <td style="padding: 10px;"><strong>Your Code</strong></td>
      <td style="padding: 10px; text-align: center;">Python (.py)</td>
      <td style="padding: 10px; text-align: center;">C++ (.cpp)</td>
    </tr>
    <tr style="border-bottom: 1px solid var(--border-color);">
      <td style="padding: 10px;"><strong>Language Runtime</strong></td>
      <td style="padding: 10px; text-align: center;">Python Interpreter ‚ö†Ô∏è</td>
      <td style="padding: 10px; text-align: center;">None (compiled) ‚úÖ</td>
    </tr>
    <tr style="border-bottom: 1px solid var(--border-color);">
      <td style="padding: 10px;"><strong>API Bindings</strong></td>
      <td style="padding: 10px; text-align: center;">pybind11 wrapper ‚ö†Ô∏è</td>
      <td style="padding: 10px; text-align: center;">Direct C++ API ‚úÖ</td>
    </tr>
    <tr style="border-bottom: 1px solid var(--border-color);">
      <td style="padding: 10px;"><strong>ONNX Runtime</strong></td>
      <td style="padding: 10px; text-align: center;" colspan="2">C++ (same for both) ‚úÖ</td>
    </tr>
    <tr style="border-bottom: 1px solid var(--border-color);">
      <td style="padding: 10px;"><strong>Execution Provider</strong></td>
      <td style="padding: 10px; text-align: center;" colspan="2">CUDA/cuDNN/CPU (same) ‚úÖ</td>
    </tr>
  </tbody>
</table>
<p><strong>Key insight:</strong> Both use the same ONNX Runtime and execution providers (CUDA, etc.). The difference is in the <em>layers above</em> - Python adds interpretation and binding overhead, C++ goes direct.</p>
</div>



<h2>The Implementation Journey: Converting PyTorch to ONNX</h2>

<p>Now that we understand <em>why</em> we need ONNX + C++ for production inference, let's dive into the <em>how</em>. Converting a PyTorch LLM to ONNX isn't as simple as calling <code>torch.onnx.export()</code> - you'll encounter several challenges along the way.</p>

<p>This section covers the real-world problems I faced when converting Qwen models from PyTorch to ONNX, and the solutions that made it work. The journey involves three main parts:</p>

<ol>
  <li><strong>Part 1:</strong> Solving the dynamic cache export problem</li>
  <li><strong>Part 2:</strong> Configuring ONNX export with optimizations</li>
  <li><strong>Part 3:</strong> Building the C++ inference engine</li>
</ol>

<div class="highlight-box highlight-info">
<p><strong>Heads up:</strong> These aren't abstract concepts - they're concrete technical challenges you'll face when converting any modern transformer model to ONNX. Understanding these will save you hours of debugging.</p>
</div>



<h2>Part 1: The Dynamic Cache Problem</h2>
<h3>Initial Attempt: Direct HuggingFace Export</h3>
<p>My first attempt was to export Qwen directly using HuggingFace Transformers:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;Qwen/Qwen3-8B&quot;</span><span class="p">)</span>
<span class="n">dummy_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">dummy_input</span><span class="p">,</span>
    <span class="s2">&quot;qwen.onnx&quot;</span><span class="p">,</span>
    <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span>
    <span class="n">output_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">],</span>
    <span class="n">dynamic_axes</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;seq_len&quot;</span><span class="p">}}</span>
<span class="p">)</span>
</code></pre></div>

<p><strong>Result:</strong> ‚ùå <strong>Failed with Dynamic Cache Error</strong></p>
<div class="codehilite"><pre><span></span><code><span class="n">RuntimeError</span><span class="o">:</span><span class="w"> </span><span class="n">Trying</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="k">export</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n n-Quoted">`DynamicCache`</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">current</span><span class="w"> </span><span class="n">version</span><span class="w"> </span>
<span class="k">of</span><span class="w"> </span><span class="n">ONNX</span><span class="w"> </span><span class="n">doesn</span><span class="s1">&#39;t support dynamic control flow. Please open an issue at </span>
<span class="s1">https://github.com/pytorch/pytorch/issues</span>
</code></pre></div>

<h3>The Root Cause</h3>
<p>Modern transformer models use <strong>KV-cache</strong> (key-value cache) to avoid recomputing attention for previously processed tokens. HuggingFace's default implementation uses <code>DynamicCache</code>, which involves:</p>
<ul>
<li>Python dictionaries</li>
<li>Dynamic list operations</li>
<li>Runtime-dependent control flow</li>
</ul>
<p>None of these translate cleanly to ONNX's static graph format.</p>
<h3>The Solution: Custom Forward Pass</h3>
<p>The fix required implementing a custom <code>forward()</code> function that:
1. <strong>Manages KV-cache explicitly</strong> as input/output tensors
2. <strong>Uses static operations</strong> (no dynamic lists or dicts)
3. <strong>Handles cache concatenation</strong> manually</p>
<p>Here's the key insight - instead of letting HuggingFace manage the cache internally, we expose it as model inputs and outputs.</p>

<div class="highlight-box highlight-success">
<p><strong>Implementation Reference:</strong> See the complete <code>QWENWrapper</code> class implementation in <a href="https://github.com/sgowdaks/llm-inference/blob/main/src/exporter.py#L64" target="_blank">src/exporter.py</a></p>
</div>

<h2>Part 2: ONNX Export with Optimizations</h2>

<div class="highlight-box highlight-success">
<p><strong>Implementation Reference:</strong> See the complete export function <code>export_to_onnx(config: ExportConfig)</code> in <a href="https://github.com/sgowdaks/llm-inference/blob/main/src/exporter.py#L141" target="_blank">src/exporter.py#L141</a></p>
</div>

<h3>Export Configuration</h3>
<p>With the custom forward pass, export becomes straightforward</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">export_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">output_path</span><span class="p">):</span>
    <span class="n">wrapped_model</span> <span class="o">=</span> <span class="n">QWENWrapper</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="c1"># Prepare dummy inputs</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">seq_len</span> <span class="o">=</span> <span class="mi">8</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span>
    <span class="n">num_kv_heads</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_key_value_heads</span>
    <span class="n">head_dim</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">//</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>

    <span class="n">dummy_input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">dummy_history_len</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="n">dummy_ids_len</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">seq_len</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="n">dummy_attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>

    <span class="c1"># Empty KV caches</span>
    <span class="n">dummy_past_kvs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span> <span class="o">*</span> <span class="mi">2</span><span class="p">):</span>  <span class="c1"># keys and values</span>
        <span class="n">dummy_past_kvs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_kv_heads</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">dummy_input_ids</span><span class="p">,</span> <span class="n">dummy_history_len</span><span class="p">,</span> <span class="n">dummy_ids_len</span><span class="p">,</span> 
              <span class="n">dummy_attention_mask</span><span class="p">,</span> <span class="o">*</span><span class="n">dummy_past_kvs</span><span class="p">)</span>

    <span class="c1"># Export with dynamic axes</span>
    <span class="n">dynamic_axes</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;seq_len&quot;</span><span class="p">},</span>
    <span class="p">}</span>

    <span class="c1"># Add dynamic axes for all KV caches</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
        <span class="n">dynamic_axes</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;past_key_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">2</span><span class="p">:</span> <span class="s2">&quot;past_seq_len&quot;</span><span class="p">}</span>
        <span class="n">dynamic_axes</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;past_value_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">2</span><span class="p">:</span> <span class="s2">&quot;past_seq_len&quot;</span><span class="p">}</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span>
        <span class="n">wrapped_model</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">output_path</span><span class="p">,</span>
        <span class="n">export_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <br><span class="n">opset_version</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span>  <span class="c1"># Important: 14+ has GPU compatibility issues</span></br>
        <span class="n">do_constant_folding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">,</span> <span class="s2">&quot;history_len&quot;</span><span class="p">,</span> <span class="s2">&quot;ids_len&quot;</span><span class="p">,</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">+</span> 
                    <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;past_key_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)]</span> <span class="o">+</span>
                    <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;past_value_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)],</span>
        <span class="n">output_names</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;out_key_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)]</span> <span class="o">+</span>
                     <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;out_value_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)]</span> <span class="o">+</span>
                     <span class="p">[</span><span class="s2">&quot;max_logit_id&quot;</span><span class="p">,</span> <span class="s2">&quot;kv_seq_len&quot;</span><span class="p">],</span>
        <span class="n">dynamic_axes</span><span class="o">=</span><span class="n">dynamic_axes</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>

<!-- <h3>Why Opset 13?</h3>
<p>Initially, I used opset 17, but encountered:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Error</span><span class="o">:</span><span class="w"> </span><span class="n">Could</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">implementation</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">Mul</span><span class="o">(</span><span class="mi">14</span><span class="o">)</span><span class="w"> </span><span class="n">node</span>
</code></pre></div> -->

<p><strong>Issue:</strong> ONNX Runtime's GPU builds don't include all CPU fallback kernels for newer opsets.</p>
<p><strong>Solution:</strong> Use opset 13 for maximum compatibility.</p>

<h2>Part 3: Building the C++ Inference Engine</h2>
<!-- <h3>Architecture Overview</h3> -->
<p>The C++ engine uses: <br>
- <strong>ONNX Runtime 1.19.0</strong> (GPU build) <br>
- <strong>HuggingFace Tokenizers (C++ bindings)</strong> for fast tokenization <br>
- <strong>nlohmann/json</strong> for configuration <br>
- <strong>CUDA + cuDNN 9</strong> for GPU acceleration</p> 

<h3>Key Implementation Challenges</h3>
<!-- <h4>Challenge 0: The C++ Tokenizer Problem</h4> -->
<p><strong>The Problem:</strong> HuggingFace doesn't provide official C++ tokenizer bindings. While Python developers can directly use <code>transformers.AutoTokenizer</code>, C++ developers face a critical gap in the inference pipeline.</p>



<p><strong>The Solution: HuggingFace Tokenizers C++ Bindings</strong></p>
<p>I use <a href="https://github.com/thammegowda/tokenizers" target="_blank">thammegowda/tokenizers</a>, a C++ binding for HuggingFace tokenizers that has an <a href="https://github.com/huggingface/tokenizers/pull/1600" target="_blank">open pull request</a> to the official HuggingFace tokenizers repository. This provides:</p>
<ul>
<li>‚úÖ Full compatibility with HuggingFace <code>tokenizer.json</code> files</li>
<li>‚úÖ Fast C++ implementation (no Python overhead)</li>
<li>‚úÖ Supports all Qwen tokenizer features (special tokens, vocab, etc.)</li>
<li>‚úÖ Seamless CMake integration</li>
</ul>

<!-- <p><strong>Testing the Tokenizer Integration:</strong></p> -->
<!-- <p>Before integrating into the main inference pipeline, I validated the tokenizer through a multi-stage testing process:</p>
<ol>
<li><strong>Standalone validation:</strong> Built a <a href="https://github.com/sgowdaks/llm-inference/tree/main/tokenizer_test_proj" target="_blank">dedicated test project</a> to verify tokenizer functionality in isolation.</li>
<li><strong>Quick integration test:</strong> A fast validation script <a href="https://github.com/sgowdaks/llm-inference/blob/main/scripts/quick_test.sh" target="_blank">scripts/quick_test.sh</a> that runs the C++ inference binary in the background, waits for model loading (~60 seconds), and confirms ONNX Runtime initializes and inference starts successfully before terminating. This catches loading/initialization issues without waiting for full generation.</li>
<li><strong>Full inference test:</strong> Validated end-to-end with <a href="https://github.com/sgowdaks/llm-inference/blob/main/scripts/test_inference.sh" target="_blank">scripts/test_inference.sh</a> - a comprehensive test that performs pre-flight checks (executable, model, tokenizer existence), runs complete inference with a 3-minute timeout, and logs output for debugging. This confirms the entire pipeline works correctly from prompt to generated response.</li>
</ol>
<p>This incremental testing approach ensured the tokenizer worked correctly before committing to the full integration.</p>

<p><strong>Integration Steps:</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. Add HuggingFace tokenizers C++ bindings as a git submodule</span>
git<span class="w"> </span>submodule<span class="w"> </span>add<span class="w"> </span>https://github.com/thammegowda/tokenizers.git<span class="w"> </span>tokenizers
git<span class="w"> </span>submodule<span class="w"> </span>update<span class="w"> </span>--init<span class="w"> </span>--recursive

<span class="c1"># 2. CMake automatically finds and links the library</span>
<span class="c1"># (no manual configuration needed)</span>

<span class="c1"># 3. Place tokenizer.json from your HuggingFace model</span>
cp<span class="w"> </span>~/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/.../tokenizer.json<span class="w"> </span>./model/
</code></pre></div>

<p><strong>Usage in C++:</strong></p>
<div class="codehilite"><pre><span></span><code><span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;tokenizers/tokenizers.h&quot;</span>

<span class="c1">// Load tokenizer from HuggingFace tokenizer.json file path</span>
<span class="k">auto</span><span class="w"> </span><span class="n">tokenizer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tokenizers</span><span class="o">::</span><span class="n">Tokenizer</span><span class="o">::</span><span class="n">FromFile</span><span class="p">(</span><span class="s">&quot;/path/to/Qwen3-8B/tokenizer.json&quot;</span><span class="p">);</span>

<span class="c1">// Encode text to token IDs</span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">prompt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;What is the capital of France?&quot;</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">token_ids</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tokenizer</span><span class="o">-&gt;</span><span class="n">Encode</span><span class="p">(</span><span class="n">prompt</span><span class="p">);</span>

<span class="c1">// Decode token IDs back to text</span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">decoded</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tokenizer</span><span class="o">-&gt;</span><span class="n">Decode</span><span class="p">(</span><span class="n">token_ids</span><span class="p">);</span>

<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Token IDs: &quot;</span><span class="p">;</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">id</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">token_ids</span><span class="p">)</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">id</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">Decoded: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">decoded</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</code></pre></div> -->

<p>The git submodule approach gives you production-grade tokenization with minimal integration effort.</p>

<h2>Part 4: CPU vs GPU Execution - Understanding the Difference</h2>

<p>The C++ inference engine supports both CPU and GPU execution. While the core ONNX model and inference logic remain the same, <strong>how you run the executable differs significantly</strong> depending on which hardware you're using.</p>

<h3>Key Differences: CPU vs GPU Execution</h3>

<div class="comparison-box">
  <div class="comparison-item">
    <h4>üíª CPU Execution</h4>
    <div class="codehilite"><pre><span></span><code><span class="c1"># Simple and direct - just run it</span>
./build/bin/onnx_inference "What is AI?"
</code></pre></div>
    <p><strong>What happens:</strong></p>
    <ul>
      <li>‚úÖ No special environment setup needed</li>
      <li>‚úÖ Uses default system libraries</li>
      <li>‚úÖ ONNX Runtime automatically uses CPU execution provider</li>
      <li>‚úÖ Works out of the box after compilation</li>
    </ul>
  </div>

  <div class="comparison-item">
    <h4>üéÆ GPU Execution (CUDA)</h4>
    <div class="codehilite"><pre><span></span><code><span class="c1"># Requires wrapper script for library setup</span>
./scripts/run_gpu_inference.sh "What is AI?"
</code></pre></div>
    <p><strong>What happens:</strong></p>
    <ul>
      <li>‚ö†Ô∏è Needs CUDA/cuDNN libraries properly configured</li>
      <li>‚ö†Ô∏è Requires setting environment variables</li>
      <li>‚ö†Ô∏è Must resolve library conflicts</li>
      <li>‚úÖ Much faster inference (10-20x speedup)</li>
    </ul>
  </div>
</div>

<h3>Setting Up GPU Inference</h3>

<h4>1. Install cuDNN 9</h4>
<p>cuDNN (CUDA Deep Neural Network library) provides GPU-accelerated implementations of neural network operations. It's essential for fast GPU inference.</p>

<div class="codehilite"><pre><span></span><code><span class="c1"># Install via conda (recommended - handles dependencies automatically)</span>
conda<span class="w"> </span>install<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span><span class="nv">cudnn</span><span class="o">=</span><span class="m">9</span>

<span class="c1"># This installs cuDNN to: $CONDA_PREFIX/lib/</span>
<span class="c1"># But the system doesn't know to look there by default!</span>
</code></pre></div>

<h4>2. The Wrapper Script Solution</h4>
<p>To make GPU execution work, we use a wrapper script (<code>scripts/run_gpu_inference.sh</code>) that sets up the environment <em>before</em> running the executable:</p>

<div class="codehilite"><pre><span></span><code><span class="ch">#!/bin/bash</span>
<span class="c1"># scripts/run_gpu_inference.sh</span>

<span class="c1"># Tell the system where to find CUDA/cuDNN libraries</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$CONDA_PREFIX</span>/lib:<span class="nv">$LD_LIBRARY_PATH</span>

<span class="c1"># Force loading the correct C++ standard library to avoid version conflicts</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_PRELOAD</span><span class="o">=</span>/usr/lib/x86_64-linux-gnu/libstdc++.so.6

<span class="c1"># Now run the actual executable with GPU support</span>
./build/bin/onnx_inference<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$@</span><span class="s2">&quot;</span>
</code></pre></div>

<h4>3. Configure CUDA Provider in C++</h4>
<p>In the C++ code, we configure ONNX Runtime to use the CUDA execution provider:</p>

<div class="codehilite"><pre><span></span><code><span class="n">OrtCUDAProviderOptions</span><span class="w"> </span><span class="n">cuda_options</span><span class="p">;</span>
<span class="n">cuda_options</span><span class="p">.</span><span class="n">device_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w">  </span><span class="c1">// Use GPU 0 (first GPU)</span>
<span class="n">cuda_options</span><span class="p">.</span><span class="n">cudnn_conv_algo_search</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">OrtCudnnConvAlgoSearchExhaustive</span><span class="p">;</span><span class="w">  </span><span class="c1">// Find best convolution algorithm</span>
<span class="n">cuda_options</span><span class="p">.</span><span class="n">gpu_mem_limit</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">SIZE_MAX</span><span class="p">;</span><span class="w">  </span><span class="c1">// No memory limit - use all available GPU RAM</span>
<span class="n">cuda_options</span><span class="p">.</span><span class="n">arena_extend_strategy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w">  </span><span class="c1">// How to allocate GPU memory</span>
<span class="n">cuda_options</span><span class="p">.</span><span class="n">do_copy_in_default_stream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w">  </span><span class="c1">// Use default CUDA stream for copies</span>

<span class="c1">// Tell ONNX Runtime to use CUDA for execution</span>
<span class="n">session_options</span><span class="p">.</span><span class="n">AppendExecutionProvider_CUDA</span><span class="p">(</span><span class="n">cuda_options</span><span class="p">);</span>
</code></pre></div>




<h2>Current Status: Functional But Underperforming üöß</h2>

<p>After navigating through dynamic cache challenges, ONNX export complexities, and C++ integration hurdles, the inference engine is now <strong>fully operational and producing correct outputs</strong>. However, initial benchmarking reveals performance is lower than expected. This is an opportunity to identify and fix optimization bottlenecks.</p>

<h3>What's Working</h3>

<div class="highlight-box highlight-success">
<ul>
  <li>‚úÖ <strong>ONNX Export:</strong> Qwen models successfully exported with custom forward pass</li>
  <li>‚úÖ <strong>C++ Inference Engine:</strong> Complete implementation with ONNX Runtime 1.19.0</li>
  <li>‚úÖ <strong>Tokenizer Integration:</strong> HuggingFace tokenizers working via C++ bindings</li>
  <li>‚úÖ <strong>GPU Acceleration:</strong> CUDA execution provider with cuDNN 9</li>
  <li>‚úÖ <strong>KV Cache Management:</strong> Efficient cache handling across iterations</li>
  <li>‚úÖ <strong>End-to-End Pipeline:</strong> Input text ‚Üí tokens ‚Üí inference ‚Üí detokenization ‚Üí output</li>
  <li>‚úÖ <strong>Correctness:</strong> Generated text is coherent and accurate</li>
</ul>
</div>

<h3>Performance Benchmarking Results</h3>

<p>Here are the actual benchmarks comparing different inference approaches:</p>

<div class="table-container">
<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>Tokens/sec</th>
      <th>Speedup vs C++</th>
      <th>Status</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>vLLM GPU (0.8)</strong></td>
      <td><strong>40.34</strong> ‚≠ê</td>
      <td>2.39x</td>
      <td>Baseline (optimized)</td>
    </tr>
    <tr>
      <td>Python ONNX (0.8)</td>
      <td>21.78</td>
      <td>1.29x</td>
      <td>Good</td>
    </tr>
    <tr>
      <td><strong>C++ ONNX GPU</strong></td>
      <td><strong>16.84</strong></td>
      <td>1.0x</td>
      <td>Needs optimization</td>
    </tr>
  </tbody>
</table>
</div>

<p><strong>Key Observations:</strong></p>
<ul>
  <li>üî¥ C++ is <strong>22% slower</strong> than Python ONNX despite having zero Python overhead in theory</li>
  <li>üî¥ Both are significantly slower than vLLM (specialized inference framework)</li>
  <li>‚ö†Ô∏è This suggests optimization opportunities in tensor management, CUDA stream usage, or ONNX Runtime configuration</li>
  <li>‚úÖ The engine produces correct outputs, so performance gap is likely not algorithmic</li>
</ul>

<!-- <h3>Identified Optimization Opportunities</h3>

<div class="highlight-box highlight-warning">
<p><strong>Why is C++ slower than Python?</strong> Possible causes being investigated:</p>
<ol>
  <li><strong>Memory copies:</strong> Unnecessary GPU ‚Üî CPU transfers in the inference loop</li>
  <li><strong>ONNX Runtime configuration:</strong> May not be using optimal graph optimization settings</li>
  <li><strong>CUDA stream management:</strong> Potential synchronization inefficiencies</li>
  <li><strong>Tensor allocation strategy:</strong> Overhead in creating/destroying tensors each iteration</li>
  <li><strong>KV cache implementation:</strong> Manual cache concatenation might be suboptimal</li>
  <li><strong>Library overhead:</strong> Tokenizer integration or binding layer overhead</li>
</ol>
</div> -->

<h3>Next Steps: Active Optimization Work</h3>

<p>I'm actively working on improving performance. The optimization roadmap includes:</p>

<!-- <div class="comparison-box">
  <div class="comparison-item">
    <h4>üéØ Short-term (High Impact)</h4>
    <ul>
      <li>Profile CUDA kernels to identify bottlenecks</li>
      <li>Optimize tensor memory pooling to reduce allocation overhead</li>
      <li>Enable ONNX Runtime graph optimization passes</li>
      <li>Reduce Python‚ÜîC++ boundary crossings in tokenizer</li>
      <li>Benchmark prefill vs decode phases separately</li>
    </ul>
  </div>
  <div class="comparison-item">
    <h4>üìà Medium-term (Architecture)</h4>
    <ul>
      <li>Implement batch processing for concurrent requests</li>
      <li>Add continuous batching (decode multiple tokens in parallel)</li>
      <li>Custom CUDA kernels for frequent operations</li>
      <li>Investigate static memory allocation strategies</li>
    </ul>
  </div>
</div> -->

<div class="highlight-box highlight-info">
<p><strong>Contribute or Track Progress:</strong> The complete code is available in the <a href="https://github.com/sgowdaks/llm-inference">GitHub repository</a>. If you have insights on optimization or find bottlenecks, please open an issue! This is an open optimization problem and community input is valuable.</p>
</div>


    </div>

    <div class="post-footer-nav">
      <a href="kv-caching-llm-inference-optimization.html" class="nav-btn nav-btn-prev">
        <i class="fas fa-arrow-left"></i>
        <div class="nav-btn-text">
          <span class="nav-label">Previous Post</span>
          <span class="nav-title">KV Caching: The Hidden Trick Behind Fast LLM Inference</span>
        </div>
      </a>
      <a href="positional-embeddings-transformers.html" class="nav-btn nav-btn-next">
        <div class="nav-btn-text">
          <span class="nav-label">Next Post</span>
          <span class="nav-title">Positional Embeddings in Transformers</span>
        </div>
        <i class="fas fa-arrow-right"></i>
      </a>
    </div>

    <div class="post-tags">
      <span class="tag">LLM</span>
      <span class="tag">ONNX</span>
      <span class="tag">C++</span>
      <span class="tag">GPU</span>
      <span class="tag">Performance</span>
      <span class="tag">AI</span>
    </div>

  </div>

  <!-- Footer (auto-generated by common.js) -->
  <footer></footer>

  <!-- Scripts -->
  <script src="../js/common.js"></script>
  
  <!-- Table of Contents Auto-Generator -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      console.log('TOC Script loaded');
      const tocList = document.getElementById('toc-list');
      const postContent = document.querySelector('.post-content');
      const headings = postContent.querySelectorAll('h2');
      
      console.log('Found ' + headings.length + ' headings');
      
      // Generate TOC items
      headings.forEach((heading, index) => {
        // Create ID for heading if it doesn't have one
        if (!heading.id) {
          heading.id = 'section-' + index;
        }
        
        // Create TOC list item
        const li = document.createElement('li');
        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;
        a.addEventListener('click', function(e) {
          e.preventDefault();
          heading.scrollIntoView({ behavior: 'smooth', block: 'start' });
          
          // Update active state
          document.querySelectorAll('.toc-list a').forEach(link => link.classList.remove('active'));
          a.classList.add('active');
        });
        
        li.appendChild(a);
        tocList.appendChild(li);
      });
      
      console.log('TOC generated successfully');
      
      // Highlight active section on scroll
      window.addEventListener('scroll', function() {
        let current = '';
        
        headings.forEach(heading => {
          const sectionTop = heading.offsetTop;
          const scrollPos = window.scrollY + 150;
          
          if (scrollPos >= sectionTop) {
            current = heading.id;
          }
        });
        
        document.querySelectorAll('.toc-list a').forEach(a => {
          a.classList.remove('active');
          if (a.getAttribute('href') === '#' + current) {
            a.classList.add('active');
          }
        });
      });
    });
  </script>
</body>
</html>
