<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Building a High-Performance ONNX Inference Engine for Qwen LLMs - Shivani Gowda KS</title>
  <link href="https://fonts.googleapis.com/css2?family=Source+Sans+3:wght@400;500;600;700&family=Source+Serif+4:wght@500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  
  <!-- Common Styles -->
  <link rel="stylesheet" href="../css/common.css">
  <link rel="stylesheet" href="../css/blog-post.css">
  
  <style>
    :root {
      --primary-color: #333333;
      --secondary-color: #555555;
      --accent-color: #666666;
      --text-color: #222222;
      --text-light: #666666;
      --bg-primary: #ffffff;
      --bg-secondary: #f8f9fa;
      --border-color: #dee2e6;
      --shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
      --gradient: linear-gradient(135deg, var(--primary-color), var(--secondary-color));
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Source Sans 3', 'Helvetica Neue', Arial, sans-serif;
      background: var(--bg-primary);
      color: var(--text-color);
      line-height: 1.7;
    }

    h1, h2, h3, h4, h5, h6 {
      font-family: 'Source Serif 4', 'Georgia', serif;
      font-weight: 600;
      color: var(--text-color);
    }

    .container {
      max-width: 800px;
      margin: 0 auto;
      padding: 2rem;
    }

    .post-header {
      text-align: center;
      margin-bottom: 3rem;
      padding-bottom: 2rem;
      border-bottom: 1px solid var(--border-color);
    }

    .post-title {
      font-size: 2.5rem;
      margin-bottom: 1rem;
      line-height: 1.2;
    }

    .post-subtitle {
      font-style: italic;
      color: var(--text-light);
      margin-bottom: 1.5rem;
      font-size: 1.1rem;
    }

    .post-meta {
      color: var(--text-light);
      font-size: 0.95rem;
      display: flex;
      justify-content: center;
      gap: 1.5rem;
      flex-wrap: wrap;
    }

    .post-category {
      background: var(--accent-color);
      color: white;
      padding: 6px 14px;
      border-radius: 15px;
      font-size: 0.8rem;
      font-weight: 600;
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }

    .post-content {
      font-size: 1.1rem;
      line-height: 1.8;
      margin-bottom: 3rem;
    }

    .post-content h2 {
      font-size: 1.8rem;
      margin: 2rem 0 1rem 0;
      color: var(--primary-color);
    }

    .post-content h3 {
      font-size: 1.4rem;
      margin: 1.5rem 0 0.8rem 0;
      color: var(--primary-color);
    }

    .post-content p {
      margin-bottom: 1.5rem;
    }

    .post-content ul, .post-content ol {
      margin: 1rem 0 1.5rem 2rem;
    }

    .post-content li {
      margin-bottom: 0.5rem;
    }

    .post-content blockquote {
      border-left: 4px solid var(--primary-color);
      padding-left: 1.5rem;
      margin: 1.5rem 0;
      font-style: italic;
      color: var(--text-light);
      background: var(--bg-secondary);
      padding: 1rem 1rem 1rem 2rem;
      border-radius: 4px;
    }

    .post-content code {
      background: var(--bg-secondary);
      padding: 0.2rem 0.4rem;
      border-radius: 4px;
      font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
      font-size: 0.9em;
      color: #d73a49;
    }

    .post-content pre {
      background: #f6f8fa;
      padding: 1.5rem;
      border-radius: 8px;
      overflow-x: auto;
      margin: 1.5rem 0;
      border: 1px solid var(--border-color);
      font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
      font-size: 0.85rem;
      line-height: 1.4;
    }

    .post-content pre code {
      background: none;
      padding: 0;
      color: #24292e;
    }

    .post-tags {
      display: flex;
      flex-wrap: wrap;
      gap: 0.5rem;
      margin-bottom: 2rem;
      justify-content: center;
    }

    .tag {
      background: var(--bg-secondary);
      color: var(--text-color);
      padding: 6px 12px;
      border-radius: 15px;
      font-size: 0.8rem;
      border: 1px solid var(--border-color);
    }

    .back-to-blog {
      text-align: center;
      padding: 2rem 0;
      border-top: 1px solid var(--border-color);
    }

    .btn {
      display: inline-flex;
      align-items: center;
      gap: 0.5rem;
      padding: 12px 24px;
      background: var(--gradient);
      color: white;
      text-decoration: none;
      border-radius: 8px;
      font-weight: 500;
      transition: all 0.3s ease;
    }

    .btn:hover {
      transform: translateY(-2px);
      box-shadow: 0 8px 25px rgba(0, 0, 0, 0.2);
    }

    .post-content a {
      color: var(--primary-color);
      text-decoration: underline;
      transition: color 0.3s ease;
    }

    .post-content a:hover {
      color: var(--accent-color);
    }

    .table-container {
      overflow-x: auto;
      margin: 1.5rem 0;
    }

    .post-content table {
      width: 100%;
      border-collapse: collapse;
      margin: 1.5rem 0;
      font-size: 0.95rem;
    }

    .post-content table th,
    .post-content table td {
      border: 1px solid var(--border-color);
      padding: 12px;
      text-align: left;
    }

    .post-content table th {
      background: var(--bg-secondary);
      font-weight: 600;
      color: var(--primary-color);
    }

    .post-content table tbody tr:nth-child(even) {
      background: #f9f9f9;
    }

    .visual-box {
      background: var(--bg-secondary);
      border: 2px solid var(--border-color);
      border-radius: 8px;
      padding: 1.5rem;
      margin: 2rem 0;
      font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
      font-size: 0.9rem;
      line-height: 1.6;
    }

    .comparison-box {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 1rem;
      margin: 2rem 0;
    }

    .comparison-item {
      background: var(--bg-secondary);
      border: 1px solid var(--border-color);
      border-radius: 8px;
      padding: 1rem;
    }

    .comparison-item h4 {
      color: var(--primary-color);
      margin-bottom: 0.5rem;
      font-size: 1rem;
    }

    .highlight-box {
      background: #fff3cd;
      border: 1px solid #ffeaa7;
      border-radius: 8px;
      padding: 1rem;
      margin: 1.5rem 0;
    }

    .highlight-box.danger {
      background: #f8d7da;
      border-color: #f5c6cb;
    }

    .highlight-box.success {
      background: #d4edda;
      border-color: #c3e6cb;
    }

    @media (max-width: 768px) {
      .container {
        padding: 1rem;
      }
      
      .post-title {
        font-size: 2rem;
      }
      
      .post-meta {
        flex-direction: column;
        align-items: center;
        gap: 0.8rem;
      }
      
      .post-content {
        font-size: 1rem;
      }
      
      .post-content pre {
        font-size: 0.8rem;
        padding: 1rem;
      }
      
      .visual-box {
        font-size: 0.8rem;
        padding: 1rem;
      }

      .comparison-box {
        grid-template-columns: 1fr;
      }
    }
  </style>
</head>

<body>
  <!-- Header (auto-generated by common.js) -->
  <header></header>

  <!-- Table of Contents Sidebar -->
  <nav class="toc-sidebar" id="toc-sidebar">
    <div class="toc-title">Table of Contents</div>
    <ul class="toc-list" id="toc-list">
      <!-- Auto-generated by script -->
    </ul>
  </nav>

  <div class="container">
    <div class="post-header">
      <h1 class="post-title">Building a High-Performance ONNX Inference Engine for LLMs</h1>
      <div class="post-subtitle">From PyTorch to C++ with GPU Acceleration</div>
      <div class="post-meta">
        <span class="post-category">Technology</span>
        <span><i class="far fa-calendar"></i> November 20, 2025</span>
        <span><i class="far fa-clock"></i> 10 min read</span>
      </div>
    </div>

    <div class="post-content">
<!-- <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-bottom: 2rem;">[Disclaimer: This blog was written solely for my understanding purpose only. Any mistakes found that need to be addressed, please feel free to reach out to me.]</p> -->
 <p style="font-size: 0.8rem; color: #666; font-style: italic; margin-bottom: 2rem;">[Disclaimer: This blog Is a work in progress]</p>
      <h1>Building a High-Performance ONNX Inference Engine for Qwen LLMs: From PyTorch to C++ with GPU Acceleration</h1>
<p><em>A deep dive into exporting Qwen language models to ONNX and building a production-ready C++ inference engine</em></p>

<h2>TL;DR</h2>
<p>I built a high-performance ONNX inference engine for Qwen language models that achieves <strong>12+ tokens/sec on GPU</strong> with C++ and CUDA. This article covers the entire journey: from dealing with HuggingFace Transformers' dynamic cache limitations, to implementing a custom forward pass, exporting to ONNX, fixing ONNX Runtime API issues, and optimizing GPU inference with cuDNN 9.</p>
<p><strong>GitHub Repository:</strong> <a href="https://github.com/sgowdaks/llm-inference">llm-inference</a></p>
<div class="highlight-box highlight-info">
<p><strong>Attribution:</strong> The custom forward pass implementation and ONNX export approach were adapted from <a href="https://github.com/DakeQQ/Native-LLM-for-Android" target="_blank">DakeQQ/Native-LLM-for-Android</a>, an excellent project for running LLMs on mobile devices. I've extended and modified these techniques for server-side GPU inference.</p>
</div>

<h2>The Challenge: Running LLMs Efficiently</h2>

<p>Large Language Models (LLMs) are powerful but resource-intensive. While Python frameworks like HuggingFace Transformers and PyTorch make it easy to prototype and train models, production inference deployments face significant challenges.</p>

<h3>The PyTorch Overhead Problem</h3>

<p>PyTorch is the go-to framework for training LLMs, and for good reasons - it's flexible, easy to debug, and has excellent GPU support. But here's what's happening under the hood:</p>

<div class="highlight-box highlight-warning">
<p><strong>PyTorch's Architecture:</strong></p>
<ul>
  <li><strong>Python frontend:</strong> Your code that defines models, training loops, etc.</li>
  <li><strong>pybind11 bindings:</strong> Translates Python calls to C++</li>
  <li><strong>libtorch (C++):</strong> The actual computation engine
    <ul>
      <li>ATen tensor library</li>
      <li>Autograd for gradients</li>
      <li>CUDA/cuDNN kernels for GPU acceleration</li>
    </ul>
  </li>
</ul>
<p><strong>Key insight:</strong> Even though PyTorch uses C++ and CUDA for the heavy lifting, <em>Python still orchestrates everything</em> - deciding which operations to run, when, and managing the model structure.</p>
</div>

<p>This Python orchestration layer introduces overhead:</p>
<ul>
  <li>üî¥ <strong>Interpreter:</strong> Python interprets code at runtime (no compilation)</li>
  <li>üî¥ <strong>GIL (Global Interpreter Lock):</strong> Limits true multi-threading</li>
  <li>üî¥ <strong>Dynamic graph construction:</strong> Model structure rebuilt for each forward pass</li>
  <li>üî¥ <strong>Memory overhead:</strong> Python runtime + garbage collector (~200-300MB)</li>
  <li>üî¥ <strong>Boundary crossings:</strong> Frequent Python ‚Üî C++ calls add latency</li>
</ul>

<h3>Training vs Inference: Different Requirements</h3>

<p>Here's a key insight that shapes modern ML workflows:</p>

<div class="comparison-box">
  <div class="comparison-item">
    <h4>üìö Training (Python is fine)</h4>
    <ul>
      <li><strong>Frequency:</strong> Done once or periodically</li>
      <li><strong>Priority:</strong> Flexibility and debuggability</li>
      <li><strong>Workflow:</strong> Iterative experimentation</li>
      <li><strong>Hardware:</strong> Usually on powerful GPU clusters</li>
      <li><strong>Python advantages:</strong>
        <ul>
          <li>Easy debugging (print statements, breakpoints)</li>
          <li>Rich ecosystem (data loaders, visualization)</li>
          <li>Quick iteration on model architectures</li>
          <li>Autograd makes gradient computation simple</li>
        </ul>
      </li>
    </ul>
    <p><strong>Verdict:</strong> Python overhead is acceptable - flexibility matters more</p>
  </div>

  <div class="comparison-item">
    <h4>‚ö° Inference (C++ shines)</h4>
    <ul>
      <li><strong>Frequency:</strong> Millions of times per day</li>
      <li><strong>Priority:</strong> Speed, cost, and efficiency</li>
      <li><strong>Workflow:</strong> Fixed model, just run it</li>
      <li><strong>Hardware:</strong> Cost-optimized, often edge devices</li>
      <li><strong>C++ advantages:</strong>
        <ul>
          <li>No interpreter overhead</li>
          <li>Lower memory footprint</li>
          <li>Faster startup time</li>
          <li>True multi-threading</li>
          <li>Deployable anywhere (mobile, IoT, servers)</li>
        </ul>
      </li>
    </ul>
    <p><strong>Verdict:</strong> Every millisecond counts - remove Python overhead</p>
  </div>
</div>

<div class="highlight-box highlight-success">
<p><strong>Common ML Production Pattern:</strong></p>
<ol>
  <li><strong>Train in Python/PyTorch:</strong> Leverage flexibility, debugging, rich ecosystem</li>
  <li><strong>Export to ONNX:</strong> Convert trained model to a portable, optimized format</li>
  <li><strong>Deploy with C++ ONNX Runtime:</strong> Run inference without Python overhead</li>
</ol>
<p>This gives you the best of both worlds: Python's ease for training, C++'s speed for inference.</p>
</div>

<h3>Why Training in C++ is Hard (and Rare)</h3>

<p>You might wonder: "If C++ is so fast, why not train in C++ too?"</p>

<p><strong>The reality:</strong> Training requires:</p>
<ul>
  <li>Frequent code changes (trying architectures, hyperparameters)</li>
  <li>Complex gradient computation (autograd)</li>
  <li>Rich debugging (inspecting tensors, gradients, losses)</li>
  <li>Data pipelines (augmentation, batching, sampling)</li>
  <li>Integration with visualization tools (TensorBoard, wandb)</li>
</ul>

<p>Implementing all of this in C++ is possible but <strong>extremely time-consuming</strong>. A research experiment that takes 1 day in Python might take 1-2 weeks in C++. The productivity cost outweighs the performance gain for training (which is done infrequently).</p>

<p><strong>For inference though</strong>, the model is frozen - no more experimentation. You just need to run the same computation graph millions of times. This is where C++'s performance advantage justifies the effort.</p>

<h3>What We Need for Production Inference</h3>

<p>To deploy LLMs efficiently, we need:</p>
<ul>
<li>‚úÖ <strong>Faster inference</strong> (lower latency, higher throughput)</li>
<li>‚úÖ <strong>Better resource utilization</strong> (more requests per GPU)</li>
<li>‚úÖ <strong>Lower deployment overhead</strong> (no Python interpreter, smaller containers)</li>
<li>‚úÖ <strong>Cross-platform compatibility</strong> (server, mobile, edge)</li>
<li>‚úÖ <strong>Static optimization</strong> (graph fusion, kernel selection)</li>
</ul>

<p><strong>The solution?</strong> ONNX (Open Neural Network Exchange) - a format that bridges training (Python) and inference (C++), providing a standardized, optimized way to deploy models across different runtimes and hardware.</p>

<p>But getting there isn't straightforward, especially for complex models like Qwen...</p>


<h2>What is ONNX?</h2>
<p>ONNX is an open, framework-agnostic format for representing neural networks as a computational graph. It standardizes how layers and operations are described so that models trained in one framework (e.g., PyTorch) can run efficiently across many runtimes and hardware backends.</p>

<h3>Dynamic vs Static Execution: A Graph Traversal Analogy</h3>
<p>To understand the fundamental difference between Python/PyTorch execution and ONNX, think of graph traversal algorithms like BFS (Breadth-First Search):</p>

<div class="comparison-box">
  <div class="comparison-item">
    <h4>üêç Python-style (Dynamic Execution)</h4>
    <div class="codehilite"><pre><span></span><code><span class="c1"># BFS graph built on the spot</span>
<span class="n">graph</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">2</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="mi">3</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">]}</span>
<span class="n">queue</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">while</span> <span class="n">queue</span><span class="p">:</span>
    <span class="n">node</span> <span class="o">=</span> <span class="n">queue</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">visit</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>   <span class="c1"># do something</span>
</code></pre></div>
    <p><strong>Characteristics:</strong></p>
    <ul>
      <li>Graph and traversal happen <em>together</em></li>
      <li>Flexible, but every run rebuilds or interprets the structure</li>
      <li>Python interpreter evaluates each line dynamically</li>
    </ul>
  </div>

  <div class="comparison-item">
    <h4>üìä ONNX-style (Precompiled Graph)</h4>
    <div class="codehilite"><pre><span></span><code><span class="c1"># BFS graph &quot;prebuilt&quot; in a file</span>
<span class="c1"># Runtime only plugs in start node</span>
<span class="n">start_node</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">run_precompiled_graph</span><span class="p">(</span><span class="n">start_node</span><span class="p">)</span>
</code></pre></div>
    <p><strong>Characteristics:</strong></p>
    <ul>
      <li>Graph structure <em>fixed in advance</em></li>
      <li>Runtime just feeds input ‚Üí gets output</li>
      <li>Efficient, portable, no dynamic construction</li>
    </ul>
  </div>
</div>

<div class="highlight-box highlight-success">
<p><strong>Analogy:</strong></p>
<ul>
  <li><strong>Python/PyTorch</strong> = "Draw the graph while traversing" - flexible but slower</li>
  <li><strong>ONNX</strong> = "Graph is drawn once, later you just plug in inputs" - fast and portable</li>
</ul>
<p>This is why ONNX models run faster: the computational graph is frozen at export time, and the runtime can optimize execution without worrying about dynamic changes.</p>
</div>


<h2>ONNX + Python vs ONNX + C++: Why Both Matter</h2>

<p>Now that we understand ONNX is a file format and it acts like a blueprint or recipe:</p>
<ul>
  <li>üìÑ <strong>ONNX file (.onnx)</strong> = The "blueprint" - describes what operations to run and how they connect</li>
  <li>üè≠ <strong>ONNX Runtime</strong> = The "factory" - reads the blueprint and executes it</li>
  <li>üîß <strong>Programming language (Python/C++)</strong> = How you interact with the factory</li>
</ul>

<p>The ONNX file itself doesn't run anything - it's just a standardized description of the model. You need an <strong>ONNX Runtime</strong> to actually execute it, and you need to call that runtime from <em>some</em> programming language.</p>

<h3>Python vs C++ ONNX Runtime: The Critical Difference</h3>

<div class="comparison-box">
  <div class="comparison-item">
    <h4>üêç Python + ONNX Runtime</h4>
    <div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">onnxruntime</span> <span class="k">as</span> <span class="nn">ort</span>

<span class="c1"># Load ONNX model</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">ort</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="s2">&quot;model.onnx&quot;</span><span class="p">)</span>

<span class="c1"># Run inference</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="n">input_data</span><span class="p">})</span>
</code></pre></div>
    <p><strong>What happens:</strong></p>
    <ol>
      <li>Python interpreter starts</li>
      <li>Loads Python ONNX Runtime bindings (pybind11 wrapper)</li>
      <li>Python ‚Üí C++ boundary crossing for <em>every</em> API call</li>
      <li>Data conversion: numpy array ‚Üí C++ tensor ‚Üí GPU memory</li>
      <li>ONNX Runtime (C++) does the actual computation</li>
      <li>Results converted back: GPU ‚Üí C++ tensor ‚Üí numpy ‚Üí Python</li>
    </ol>
    <p><strong>Overhead sources:</strong></p>
    <ul>
      <li>Python interpreter startup (~200-500ms)</li>
      <li>GIL (Global Interpreter Lock) for thread safety</li>
      <li>Python ‚Üí C++ function call overhead</li>
      <li>Data type conversions and memory copies</li>
      <li>Python object management and garbage collection</li>
    </ul>
  </div>

  <div class="comparison-item">
    <h4>‚ö° C++ + ONNX Runtime</h4>
    <div class="codehilite"><pre><span></span><code><span class="cp">#include</span> <span class="cpf">&lt;onnxruntime_cxx_api.h&gt;</span>

<span class="c1">// Load ONNX model</span>
<span class="n">Ort</span><span class="o">::</span><span class="n">Session</span> <span class="n">session</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="s">&quot;model.onnx&quot;</span><span class="p">,</span> <span class="n">options</span><span class="p">);</span>

<span class="c1">// Run inference</span>
<span class="k">auto</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">session</span><span class="p">.</span><span class="n">Run</span><span class="p">(</span><span class="n">run_options</span><span class="p">,</span> 
                           <span class="n">input_names</span><span class="p">,</span> 
                           <span class="o">&amp;</span><span class="n">input_tensor</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                           <span class="n">output_names</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</code></pre></div>
    <p><strong>What happens:</strong></p>
    <ol>
      <li>Native C++ executable starts</li>
      <li>Directly calls ONNX Runtime C++ API (no wrapper)</li>
      <li><strong>Zero</strong> language boundary crossing</li>
      <li>Direct memory access: C++ ‚Üí GPU (no conversions)</li>
      <li>ONNX Runtime does computation</li>
      <li>Results stay in C++ memory (no conversions)</li>
    </ol>
    <p><strong>Advantages:</strong></p>
    <ul>
      <li>Fast startup (~50-100ms)</li>
      <li>No GIL, true multi-threading</li>
      <li>Direct function calls (no overhead)</li>
      <li>Zero-copy operations where possible</li>
      <li>Manual memory control, no GC pauses</li>
    </ul>
  </div>
</div>

<h3>The Performance Stack</h3>

<div class="highlight-box highlight-warning">
<p><strong>Understanding the layers:</strong></p>
<table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
  <thead>
    <tr style="background: var(--bg-secondary); border-bottom: 2px solid var(--border-color);">
      <th style="padding: 10px; text-align: left;">Layer</th>
      <th style="padding: 10px; text-align: center;">Python ONNX</th>
      <th style="padding: 10px; text-align: center;">C++ ONNX</th>
    </tr>
  </thead>
  <tbody>
    <tr style="border-bottom: 1px solid var(--border-color);">
      <td style="padding: 10px;"><strong>Your Code</strong></td>
      <td style="padding: 10px; text-align: center;">Python (.py)</td>
      <td style="padding: 10px; text-align: center;">C++ (.cpp)</td>
    </tr>
    <tr style="border-bottom: 1px solid var(--border-color);">
      <td style="padding: 10px;"><strong>Language Runtime</strong></td>
      <td style="padding: 10px; text-align: center;">Python Interpreter ‚ö†Ô∏è</td>
      <td style="padding: 10px; text-align: center;">None (compiled) ‚úÖ</td>
    </tr>
    <tr style="border-bottom: 1px solid var(--border-color);">
      <td style="padding: 10px;"><strong>API Bindings</strong></td>
      <td style="padding: 10px; text-align: center;">pybind11 wrapper ‚ö†Ô∏è</td>
      <td style="padding: 10px; text-align: center;">Direct C++ API ‚úÖ</td>
    </tr>
    <tr style="border-bottom: 1px solid var(--border-color);">
      <td style="padding: 10px;"><strong>ONNX Runtime</strong></td>
      <td style="padding: 10px; text-align: center;" colspan="2">C++ (same for both) ‚úÖ</td>
    </tr>
    <tr style="border-bottom: 1px solid var(--border-color);">
      <td style="padding: 10px;"><strong>Execution Provider</strong></td>
      <td style="padding: 10px; text-align: center;" colspan="2">CUDA/cuDNN/CPU (same) ‚úÖ</td>
    </tr>
  </tbody>
</table>
<p><strong>Key insight:</strong> Both use the same ONNX Runtime and execution providers (CUDA, etc.). The difference is in the <em>layers above</em> - Python adds interpretation and binding overhead, C++ goes direct.</p>
</div>



<h2>The Implementation Journey: Converting PyTorch to ONNX</h2>

<p>Now that we understand <em>why</em> we need ONNX + C++ for production inference, let's dive into the <em>how</em>. Converting a PyTorch LLM to ONNX isn't as simple as calling <code>torch.onnx.export()</code> - you'll encounter several challenges along the way.</p>

<p>This section covers the real-world problems I faced when converting Qwen models from PyTorch to ONNX, and the solutions that made it work. The journey involves three main parts:</p>

<ol>
  <li><strong>Part 1:</strong> Solving the dynamic cache export problem</li>
  <li><strong>Part 2:</strong> Configuring ONNX export with optimizations</li>
  <li><strong>Part 3:</strong> Building the C++ inference engine</li>
</ol>

<div class="highlight-box highlight-info">
<p><strong>Heads up:</strong> These aren't abstract concepts - they're concrete technical challenges you'll face when converting any modern transformer model to ONNX. Understanding these will save you hours of debugging.</p>
</div>



<h2>Part 1: The Dynamic Cache Problem</h2>
<h3>Initial Attempt: Direct HuggingFace Export</h3>
<p>My first attempt was to export Qwen directly using HuggingFace Transformers:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;Qwen/Qwen3-8B&quot;</span><span class="p">)</span>
<span class="n">dummy_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">dummy_input</span><span class="p">,</span>
    <span class="s2">&quot;qwen.onnx&quot;</span><span class="p">,</span>
    <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span>
    <span class="n">output_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">],</span>
    <span class="n">dynamic_axes</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;seq_len&quot;</span><span class="p">}}</span>
<span class="p">)</span>
</code></pre></div>

<p><strong>Result:</strong> ‚ùå <strong>Failed with Dynamic Cache Error</strong></p>
<div class="codehilite"><pre><span></span><code><span class="n">RuntimeError</span><span class="o">:</span><span class="w"> </span><span class="n">Trying</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="k">export</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n n-Quoted">`DynamicCache`</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">current</span><span class="w"> </span><span class="n">version</span><span class="w"> </span>
<span class="k">of</span><span class="w"> </span><span class="n">ONNX</span><span class="w"> </span><span class="n">doesn</span><span class="s1">&#39;t support dynamic control flow. Please open an issue at </span>
<span class="s1">https://github.com/pytorch/pytorch/issues</span>
</code></pre></div>

<h3>The Root Cause</h3>
<p>Modern transformer models use <strong>KV-cache</strong> (key-value cache) to avoid recomputing attention for previously processed tokens. HuggingFace's default implementation uses <code>DynamicCache</code>, which involves:</p>
<ul>
<li>Python dictionaries</li>
<li>Dynamic list operations</li>
<li>Runtime-dependent control flow</li>
</ul>
<p>None of these translate cleanly to ONNX's static graph format.</p>
<h3>The Solution: Custom Forward Pass</h3>
<p>The fix required implementing a custom <code>forward()</code> function that:
1. <strong>Manages KV-cache explicitly</strong> as input/output tensors
2. <strong>Uses static operations</strong> (no dynamic lists or dicts)
3. <strong>Handles cache concatenation</strong> manually</p>
<p>Here's the key insight - instead of letting HuggingFace manage the cache internally, we expose it as model inputs and outputs.</p>

<div class="highlight-box highlight-success">
<p><strong>Implementation Reference:</strong> See the complete <code>QWENWrapper</code> class implementation in <a href="https://github.com/sgowdaks/llm-inference/blob/main/src/exporter.py#L64" target="_blank">src/exporter.py</a></p>
</div>

<!-- <p><strong>Key Points:</strong>
- <strong>72 KV tensors</strong> (36 layers √ó 2 for keys/values) + metadata = 74 outputs
- <strong>Static graph structure</strong> - no Python loops or conditionals
- <strong>Explicit cache management</strong> - client code handles passing cache between iterations</p> -->

<h2>Part 2: ONNX Export with Optimizations</h2>

<div class="highlight-box highlight-success">
<p><strong>Implementation Reference:</strong> See the complete export function <code>export_to_onnx(config: ExportConfig)</code> in <a href="https://github.com/sgowdaks/llm-inference/blob/main/src/exporter.py#L141" target="_blank">src/exporter.py#L141</a></p>
</div>

<h3>Export Configuration</h3>
<p>With the custom forward pass, export becomes straightforward</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">export_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">output_path</span><span class="p">):</span>
    <span class="n">wrapped_model</span> <span class="o">=</span> <span class="n">QWENWrapper</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="c1"># Prepare dummy inputs</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">seq_len</span> <span class="o">=</span> <span class="mi">8</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span>
    <span class="n">num_kv_heads</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_key_value_heads</span>
    <span class="n">head_dim</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">//</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>

    <span class="n">dummy_input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">dummy_history_len</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="n">dummy_ids_len</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">seq_len</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="n">dummy_attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>

    <span class="c1"># Empty KV caches</span>
    <span class="n">dummy_past_kvs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span> <span class="o">*</span> <span class="mi">2</span><span class="p">):</span>  <span class="c1"># keys and values</span>
        <span class="n">dummy_past_kvs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_kv_heads</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">dummy_input_ids</span><span class="p">,</span> <span class="n">dummy_history_len</span><span class="p">,</span> <span class="n">dummy_ids_len</span><span class="p">,</span> 
              <span class="n">dummy_attention_mask</span><span class="p">,</span> <span class="o">*</span><span class="n">dummy_past_kvs</span><span class="p">)</span>

    <span class="c1"># Export with dynamic axes</span>
    <span class="n">dynamic_axes</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;seq_len&quot;</span><span class="p">},</span>
    <span class="p">}</span>

    <span class="c1"># Add dynamic axes for all KV caches</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
        <span class="n">dynamic_axes</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;past_key_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">2</span><span class="p">:</span> <span class="s2">&quot;past_seq_len&quot;</span><span class="p">}</span>
        <span class="n">dynamic_axes</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;past_value_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">2</span><span class="p">:</span> <span class="s2">&quot;past_seq_len&quot;</span><span class="p">}</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span>
        <span class="n">wrapped_model</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">output_path</span><span class="p">,</span>
        <span class="n">export_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <br><span class="n">opset_version</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span>  <span class="c1"># Important: 14+ has GPU compatibility issues</span></br>
        <span class="n">do_constant_folding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">,</span> <span class="s2">&quot;history_len&quot;</span><span class="p">,</span> <span class="s2">&quot;ids_len&quot;</span><span class="p">,</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">+</span> 
                    <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;past_key_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)]</span> <span class="o">+</span>
                    <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;past_value_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)],</span>
        <span class="n">output_names</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;out_key_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)]</span> <span class="o">+</span>
                     <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;out_value_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)]</span> <span class="o">+</span>
                     <span class="p">[</span><span class="s2">&quot;max_logit_id&quot;</span><span class="p">,</span> <span class="s2">&quot;kv_seq_len&quot;</span><span class="p">],</span>
        <span class="n">dynamic_axes</span><span class="o">=</span><span class="n">dynamic_axes</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>

<h3>Why Opset 13?</h3>
<p>Initially, I used opset 17, but encountered:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Error</span><span class="o">:</span><span class="w"> </span><span class="n">Could</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">implementation</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">Mul</span><span class="o">(</span><span class="mi">14</span><span class="o">)</span><span class="w"> </span><span class="n">node</span>
</code></pre></div>

<p><strong>Issue:</strong> ONNX Runtime's GPU builds don't include all CPU fallback kernels for newer opsets.</p>
<p><strong>Solution:</strong> Use opset 13 for maximum compatibility.</p>

<h2>Part 3: Building the C++ Inference Engine</h2>
<!-- <h3>Architecture Overview</h3> -->
<p>The C++ engine uses: <br>
- <strong>ONNX Runtime 1.19.0</strong> (GPU build) <br>
- <strong>HuggingFace Tokenizers (C++ bindings)</strong> for fast tokenization <br>
- <strong>nlohmann/json</strong> for configuration <br>
- <strong>CUDA + cuDNN 9</strong> for GPU acceleration</p> 

<h3>Key Implementation Challenges</h3>
<!-- <h4>Challenge 0: The C++ Tokenizer Problem</h4> -->
<p><strong>The Problem:</strong> HuggingFace doesn't provide official C++ tokenizer bindings. While Python developers can directly use <code>transformers.AutoTokenizer</code>, C++ developers face a critical gap in the inference pipeline.</p>

<!-- <p><strong>Initial Considerations:</strong></p>
<ul>
<li>‚ùå <strong>Python subprocess:</strong> Massive overhead, defeats the purpose of C++ inference</li>
<li>‚ùå <strong>Manual tokenization:</strong> Complex, error-prone, incompatible with pretrained models</li>
<li>‚úÖ <strong>HuggingFace Tokenizers C++ bindings:</strong> Fast C++ library compatible with HuggingFace <code>tokenizer.json</code> format</li>
</ul> -->

<p><strong>The Solution: HuggingFace Tokenizers C++ Bindings</strong></p>
<p>I use <a href="https://github.com/thammegowda/tokenizers" target="_blank">thammegowda/tokenizers</a>, a C++ binding for HuggingFace tokenizers that has an <a href="https://github.com/huggingface/tokenizers/pull/1600" target="_blank">open pull request</a> to the official HuggingFace tokenizers repository. This provides:</p>
<ul>
<li>‚úÖ Full compatibility with HuggingFace <code>tokenizer.json</code> files</li>
<li>‚úÖ Fast C++ implementation (no Python overhead)</li>
<li>‚úÖ Supports all Qwen tokenizer features (special tokens, vocab, etc.)</li>
<li>‚úÖ Seamless CMake integration</li>
</ul>

<!-- <p><strong>Testing the Tokenizer Integration:</strong></p> -->
<p>Before integrating into the main inference pipeline, I validated the tokenizer through a multi-stage testing process:</p>
<ol>
<li><strong>Standalone validation:</strong> Built a <a href="https://github.com/sgowdaks/llm-inference/tree/main/tokenizer_test_proj" target="_blank">dedicated test project</a> to verify tokenizer functionality in isolation.</li>
<li><strong>Quick integration test:</strong> A fast validation script <a href="https://github.com/sgowdaks/llm-inference/blob/main/scripts/quick_test.sh" target="_blank">scripts/quick_test.sh</a> that runs the C++ inference binary in the background, waits for model loading (~60 seconds), and confirms ONNX Runtime initializes and inference starts successfully before terminating. This catches loading/initialization issues without waiting for full generation.</li>
<li><strong>Full inference test:</strong> Validated end-to-end with <a href="https://github.com/sgowdaks/llm-inference/blob/main/scripts/test_inference.sh" target="_blank">scripts/test_inference.sh</a> - a comprehensive test that performs pre-flight checks (executable, model, tokenizer existence), runs complete inference with a 3-minute timeout, and logs output for debugging. This confirms the entire pipeline works correctly from prompt to generated response.</li>
</ol>
<p>This incremental testing approach ensured the tokenizer worked correctly before committing to the full integration.</p>

<p><strong>Integration Steps:</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. Add HuggingFace tokenizers C++ bindings as a git submodule</span>
git<span class="w"> </span>submodule<span class="w"> </span>add<span class="w"> </span>https://github.com/thammegowda/tokenizers.git<span class="w"> </span>tokenizers
git<span class="w"> </span>submodule<span class="w"> </span>update<span class="w"> </span>--init<span class="w"> </span>--recursive

<span class="c1"># 2. CMake automatically finds and links the library</span>
<span class="c1"># (no manual configuration needed)</span>

<span class="c1"># 3. Place tokenizer.json from your HuggingFace model</span>
cp<span class="w"> </span>~/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/.../tokenizer.json<span class="w"> </span>./model/
</code></pre></div>

<p><strong>Usage in C++:</strong></p>
<div class="codehilite"><pre><span></span><code><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;tokenizers_cpp.h&gt;</span>

<span class="c1">// Load tokenizer from HuggingFace tokenizer.json</span>
<span class="k">auto</span><span class="w"> </span><span class="n">tokenizer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tokenizers</span><span class="o">::</span><span class="n">Tokenizer</span><span class="o">::</span><span class="n">FromBlobJSON</span><span class="p">(</span><span class="n">json_blob</span><span class="p">);</span>

<span class="c1">// Encode text to token IDs</span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">prompt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;What is the capital of France?&quot;</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">token_ids</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tokenizer</span><span class="o">-&gt;</span><span class="n">Encode</span><span class="p">(</span><span class="n">prompt</span><span class="p">);</span>

<span class="c1">// Decode token IDs back to text</span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">decoded</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tokenizer</span><span class="o">-&gt;</span><span class="n">Decode</span><span class="p">(</span><span class="n">token_ids</span><span class="p">);</span>
</code></pre></div>

<p>The git submodule approach gives you production-grade tokenization with minimal integration effort.</p>



<!-- <h4>Challenge 1: ONNX Runtime API Compatibility</h4>
<p>ONNX Runtime 1.19.0 changed several APIs:</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// ‚ùå Old API (pre-1.19)</span>
<span class="n">session_options</span><span class="p">.</span><span class="n">AddSessionConfigEntry</span><span class="p">(</span><span class="s">&quot;key&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;value&quot;</span><span class="p">);</span>
<span class="n">session_options</span><span class="p">.</span><span class="n">SetLogVerbosityLevel</span><span class="p">(</span><span class="mi">4</span><span class="p">);</span>
<span class="n">Ort</span><span class="o">::</span><span class="n">ThrowOnError</span><span class="p">(</span><span class="n">OrtSessionOptionsAppendExecutionProvider_CUDA</span><span class="p">(</span><span class="n">options</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">));</span>

<span class="c1">// ‚úÖ New API (1.19.0+)</span>
<span class="n">session_options</span><span class="p">.</span><span class="n">AddConfigEntry</span><span class="p">(</span><span class="s">&quot;key&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;value&quot;</span><span class="p">);</span>
<span class="c1">// SetLogVerbosityLevel removed - use session options</span>
<span class="n">OrtCUDAProviderOptions</span><span class="w"> </span><span class="n">cuda_options</span><span class="p">;</span>
<span class="n">cuda_options</span><span class="p">.</span><span class="n">device_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="n">session_options</span><span class="p">.</span><span class="n">AppendExecutionProvider_CUDA</span><span class="p">(</span><span class="n">cuda_options</span><span class="p">);</span>
</code></pre></div>

<h4>Challenge 2: Tensor Lifecycle Management</h4>
<p><strong>The Bug:</strong> Generated tokens were garbage:</p>
<div class="codehilite"><pre><span></span><code>Output:<span class="w"> </span>&quot;ƒäƒäƒäƒä<span class="nt">&lt;/think&gt;&lt;/think&gt;</span>and<span class="w"> </span>and<span class="w"> </span>and<span class="w"> </span>and...&quot;
</code></pre></div>

<p><strong>Root Cause:</strong> Data vectors went out of scope before tensor creation:</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// ‚ùå WRONG - vector destroyed after loop iteration</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">max_tokens</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">new_token</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="n">token_id</span><span class="p">};</span><span class="w">  </span><span class="c1">// Goes out of scope!</span>
<span class="w">    </span><span class="n">input_tensors</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span>
<span class="w">        </span><span class="n">Ort</span><span class="o">::</span><span class="n">Value</span><span class="o">::</span><span class="n">CreateTensor</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">memory_info</span><span class="p">,</span><span class="w"> </span>
<span class="w">            </span><span class="n">new_token</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="p">...));</span><span class="w">  </span><span class="c1">// Dangling pointer!</span>
<span class="p">}</span>
</code></pre></div>

<p><strong>Solution:</strong> Maintain persistent buffers:</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// ‚úÖ CORRECT - persistent across iterations</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">current_tokens</span><span class="p">(</span><span class="n">ids_vec</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span><span class="w"> </span><span class="n">ids_vec</span><span class="p">.</span><span class="n">end</span><span class="p">());</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">history_len_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">ids_len_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">current_tokens</span><span class="p">.</span><span class="n">size</span><span class="p">())};</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int8_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">attention_mask_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">1</span><span class="p">};</span> -->

<!-- <span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">max_tokens</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Create tensors from persistent data</span>
<span class="w">    </span><span class="n">input_tensors</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span>
<span class="w">        </span><span class="n">Ort</span><span class="o">::</span><span class="n">Value</span><span class="o">::</span><span class="n">CreateTensor</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">memory_info</span><span class="p">,</span><span class="w"> </span>
<span class="w">            </span><span class="n">current_tokens</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="n">current_tokens</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span><span class="w"> </span><span class="p">...));</span>

<span class="w">    </span><span class="c1">// ... run inference ...</span>

<span class="w">    </span><span class="c1">// Update buffers for next iteration</span>
<span class="w">    </span><span class="n">current_tokens</span><span class="p">.</span><span class="n">clear</span><span class="p">();</span>
<span class="w">    </span><span class="n">current_tokens</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">token_id</span><span class="p">));</span>
<span class="w">    </span><span class="n">history_len_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">ids_len_data</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">    </span><span class="n">ids_len_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">    </span><span class="n">attention_mask_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div> -->

<!-- <h4>Challenge 3: KV Cache Management</h4>
<p>Each decode iteration:
1. Takes 76 input tensors: 72 KV caches + 4 metadata
2. Produces 74 output tensors: 72 new KV caches + 2 outputs
3. Must reconstruct inputs from outputs</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// Move KV caches from outputs to inputs (zero-copy)</span>
<span class="n">input_tensors</span><span class="p">.</span><span class="n">clear</span><span class="p">();</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">num_layers_</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">input_tensors</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">[</span><span class="n">i</span><span class="p">]));</span>
<span class="p">}</span>

<span class="c1">// Create new metadata tensors</span>
<span class="n">input_tensors</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">Ort</span><span class="o">::</span><span class="n">Value</span><span class="o">::</span><span class="n">CreateTensor</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span><span class="p">(...));</span><span class="w">  </span><span class="c1">// token</span>
<span class="n">input_tensors</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">Ort</span><span class="o">::</span><span class="n">Value</span><span class="o">::</span><span class="n">CreateTensor</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">(...));</span><span class="w">  </span><span class="c1">// history_len</span>
<span class="n">input_tensors</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">Ort</span><span class="o">::</span><span class="n">Value</span><span class="o">::</span><span class="n">CreateTensor</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">(...));</span><span class="w">  </span><span class="c1">// ids_len</span>
<span class="n">input_tensors</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">Ort</span><span class="o">::</span><span class="n">Value</span><span class="o">::</span><span class="n">CreateTensor</span><span class="o">&lt;</span><span class="kt">int8_t</span><span class="o">&gt;</span><span class="p">(...));</span><span class="w">   </span><span class="c1">// attention_mask</span>
</code></pre></div> -->


<h2>Part 4: GPU Acceleration with CUDA</h2>
<h3>Setting Up GPU Inference</h3>
<h4>1. Install cuDNN 9</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># Via conda (recommended)</span>
conda<span class="w"> </span>install<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span><span class="nv">cudnn</span><span class="o">=</span><span class="m">9</span>

<span class="c1"># Set environment</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$CONDA_PREFIX</span>/lib:<span class="nv">$LD_LIBRARY_PATH</span>
</code></pre></div>

<h4>2. Configure CUDA Provider</h4>
<div class="codehilite"><pre><span></span><code><span class="n">OrtCUDAProviderOptions</span><span class="w"> </span><span class="n">cuda_options</span><span class="p">;</span>
<span class="n">cuda_options</span><span class="p">.</span><span class="n">device_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w">  </span><span class="c1">// Use GPU 0</span>
<span class="n">cuda_options</span><span class="p">.</span><span class="n">cudnn_conv_algo_search</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">OrtCudnnConvAlgoSearchExhaustive</span><span class="p">;</span>
<span class="n">cuda_options</span><span class="p">.</span><span class="n">gpu_mem_limit</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">SIZE_MAX</span><span class="p">;</span><span class="w">  </span><span class="c1">// No limit</span>
<span class="n">cuda_options</span><span class="p">.</span><span class="n">arena_extend_strategy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="n">cuda_options</span><span class="p">.</span><span class="n">do_copy_in_default_stream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>

<span class="n">session_options</span><span class="p">.</span><span class="n">AppendExecutionProvider_CUDA</span><span class="p">(</span><span class="n">cuda_options</span><span class="p">);</span>
</code></pre></div>

<h4>3. Handle Library Conflicts</h4>
<p>Created a wrapper script to resolve libstdc++ conflicts:</p>
<div class="codehilite"><pre><span></span><code><span class="ch">#!/bin/bash</span>
<span class="c1"># scripts/run_gpu_inference.sh</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$CONDA_PREFIX</span>/lib:<span class="nv">$LD_LIBRARY_PATH</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_PRELOAD</span><span class="o">=</span>/usr/lib/x86_64-linux-gnu/libstdc++.so.6

./build/bin/onnx_inference<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$@</span><span class="s2">&quot;</span>
</code></pre></div>



<!-- <h2>Part 6: Project Structure</h2>
<p>After refactoring, the project is organized as:</p>
<div class="codehilite"><pre><span></span><code><span class="n">llm</span><span class="o">-</span><span class="n">inference</span><span class="o">/</span>
<span class="err">‚îú‚îÄ‚îÄ</span><span class="w"> </span><span class="n">src</span><span class="o">/</span><span class="w">                    </span><span class="c1"># Source code</span>
<span class="err">‚îÇ</span><span class="w">   </span><span class="err">‚îú‚îÄ‚îÄ</span><span class="w"> </span><span class="n">onnx_inference</span><span class="o">.</span><span class="n">cpp</span><span class="w">  </span><span class="c1"># C++ inference engine</span>
<span class="err">‚îÇ</span><span class="w">   </span><span class="err">‚îú‚îÄ‚îÄ</span><span class="w"> </span><span class="n">onnx_inference</span><span class="o">.</span><span class="n">py</span><span class="w">   </span><span class="c1"># Python inference</span>
<span class="err">‚îÇ</span><span class="w">   </span><span class="err">‚îî‚îÄ‚îÄ</span><span class="w"> </span><span class="n">exporter</span><span class="o">.</span><span class="n">py</span><span class="w">         </span><span class="c1"># Model export</span>
<span class="err">‚îú‚îÄ‚îÄ</span><span class="w"> </span><span class="n">scripts</span><span class="o">/</span><span class="w">                </span><span class="c1"># Utility scripts</span>
<span class="err">‚îÇ</span><span class="w">   </span><span class="err">‚îú‚îÄ‚îÄ</span><span class="w"> </span><span class="n">run_gpu_inference</span><span class="o">.</span><span class="n">sh</span>
<span class="err">‚îÇ</span><span class="w">   </span><span class="err">‚îî‚îÄ‚îÄ</span><span class="w"> </span><span class="n">test_inference</span><span class="o">.</span><span class="n">sh</span>
<span class="err">‚îú‚îÄ‚îÄ</span><span class="w"> </span><span class="n">configs</span><span class="o">/</span><span class="w">                </span><span class="c1"># Configuration</span>
<span class="err">‚îÇ</span><span class="w">   </span><span class="err">‚îî‚îÄ‚îÄ</span><span class="w"> </span><span class="n">config</span><span class="o">.</span><span class="n">json</span>
<span class="err">‚îú‚îÄ‚îÄ</span><span class="w"> </span><span class="n">docs</span><span class="o">/</span><span class="w">                   </span><span class="c1"># Documentation</span>
<span class="err">‚îÇ</span><span class="w">   </span><span class="err">‚îú‚îÄ‚îÄ</span><span class="w"> </span><span class="n">BUILD</span><span class="o">.</span><span class="n">md</span>
<span class="err">‚îÇ</span><span class="w">   </span><span class="err">‚îú‚îÄ‚îÄ</span><span class="w"> </span><span class="n">CONTRIBUTING</span><span class="o">.</span><span class="n">md</span>
<span class="err">‚îÇ</span><span class="w">   </span><span class="err">‚îî‚îÄ‚îÄ</span><span class="w"> </span><span class="n">FIXES_APPLIED</span><span class="o">.</span><span class="n">md</span>
<span class="err">‚îú‚îÄ‚îÄ</span><span class="w"> </span><span class="n">CMakeLists</span><span class="o">.</span><span class="n">txt</span><span class="w">         </span><span class="c1"># Build system</span>
<span class="err">‚îî‚îÄ‚îÄ</span><span class="w"> </span><span class="n">requirements</span><span class="o">.</span><span class="n">txt</span><span class="w">       </span><span class="c1"># Python deps</span>
</code></pre></div> -->

<!-- 
<h2>Part 7: Lessons Learned</h2>
<h3>1. HuggingFace Abstractions Can Hide Complexity</h3>
<p>The <code>DynamicCache</code> abstraction is great for research but a barrier for deployment. Understanding the underlying mechanism is crucial.</p>
<h3>2. ONNX Opset Matters</h3>
<p>Newer isn't always better. Opset compatibility with your runtime is more important than having the latest features.</p>
<h3>3. Memory Management in C++ is Critical</h3>
<p>Modern C++ with smart pointers helps, but ONNX Runtime's C API requires careful attention to:
- Tensor data lifetime 
- Memory ownership
- Zero-copy operations</p> -->
<!-- <h3>4. GPU Acceleration Isn't Automatic</h3>
<p>Getting cuDNN working required:
- Correct library versions
- Environment variables
- Library path resolution
- Preload order</p>
<h3>5. Debugging Requires Multiple Perspectives</h3>
<p>When output was garbage, I had to check:
- Token IDs (valid range?)
- Tensor shapes (correct dimensions?)
- Data types (int32 vs int64?)
- Memory lifetimes (data still valid?)
- KV cache (properly passed?)</p>

<h2>Getting Started</h2>
<h3>Quick Start</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># Clone repository</span>
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/sgowdaks/llm-inference.git
<span class="nb">cd</span><span class="w"> </span>llm-inference
git<span class="w"> </span>submodule<span class="w"> </span>update<span class="w"> </span>--init<span class="w"> </span>--recursive -->

<!-- <span class="c1"># Install Python dependencies</span>
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt

<span class="c1"># Download ONNX Runtime GPU</span>
wget<span class="w"> </span>https://github.com/microsoft/onnxruntime/releases/download/v1.19.0/onnxruntime-linux-x64-gpu-1.19.0.tgz
tar<span class="w"> </span>-xzf<span class="w"> </span>onnxruntime-linux-x64-gpu-1.19.0.tgz

<span class="c1"># Install cuDNN</span>
conda<span class="w"> </span>install<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span><span class="nv">cudnn</span><span class="o">=</span><span class="m">9</span>

<span class="c1"># Build C++ inference</span>
mkdir<span class="w"> </span>build<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>build
cmake<span class="w"> </span>..<span class="w"> </span>-DONNXRUNTIME_ROOT_DIR<span class="o">=</span>/path/to/onnxruntime-linux-x64-gpu-1.19.0
make<span class="w"> </span>-j4

<span class="c1"># Export model</span> -->
<!-- <span class="nb">cd</span><span class="w"> </span>..
cp<span class="w"> </span>configs/config.example.json<span class="w"> </span>configs/config.json
<span class="c1"># Edit configs/config.json with your paths</span>
python<span class="w"> </span>src/exporter.py<span class="w"> </span>--config<span class="w"> </span>configs/config.json<span class="w"> </span>--mode<span class="w"> </span><span class="nb">export</span>

<span class="c1"># Run inference</span>
./scripts/run_gpu_inference.sh<span class="w"> </span><span class="s2">&quot;What is machine learning?&quot;</span>
</code></pre></div>

<h3>Expected Output</h3>
<div class="codehilite"><pre><span></span><code>Using CUDA execution provider on GPU 0

Prompt: What is machine learning?
Qwen Answering:

Machine learning is a subset of artificial intelligence (AI) that 
focuses on the development of algorithms and statistical models that 
enable computers to learn from and make predictions or decisions based 
on data...

Decode: 12.28 token/s
</code></pre></div> -->

<!-- 
<h2>Future Improvements</h2>
<h3>Planned Features</h3>
<ol>
<li><strong>Batch Inference</strong> - Process multiple prompts simultaneously</li>
<li><strong>Quantization</strong> - INT8/INT4 support for smaller memory footprint</li>
<li><strong>Multi-GPU</strong> - Distribute inference across devices</li>
<li><strong>More Models</strong> - Support Llama, Mistral, etc.</li>
<li><strong>Python Bindings</strong> - PyBind11 wrapper for C++ engine</li>
<li><strong>Streaming API</strong> - Token-by-token generation</li>
<li><strong>Request Batching</strong> - Dynamic batching for throughput</li>
</ol>
<h3>Performance Optimizations</h3>
<ul>
<li>Flash Attention integration</li>
<li>Continuous batching</li>
<li>KV cache quantization</li>
<li>Model parallelism</li>
<li>Pipeline parallelism</li> -->
<!-- </ul>

<h2>Conclusion</h2>
<p>Building a production-ready inference engine for LLMs involves much more than just exporting a model. From understanding cache management, to navigating ONNX Runtime APIs, to optimizing GPU utilization - each step presents unique challenges.</p>
<p>The result? A system that:
- ‚úÖ Runs 12+ tokens/sec on GPU
- ‚úÖ Has no Python runtime overhead
- ‚úÖ Supports both CPU and GPU
- ‚úÖ Is production-ready and maintainable</p>
<p>The journey taught me that modern ML frameworks are powerful but sometimes you need to go lower-level to achieve your goals. The trade-off between convenience and control is real, and knowing when to make that trade is key.</p>
 -->

<h2>Current Status: It Works! üéâ</h2>

<p>After navigating through dynamic cache challenges, ONNX export complexities, and C++ integration hurdles, the inference engine is now <strong>fully operational</strong>.</p>

<h3>What's Working</h3>

<div class="highlight-box highlight-success">
<ul>
  <li>‚úÖ <strong>ONNX Export:</strong> Qwen models successfully exported with custom forward pass</li>
  <li>‚úÖ <strong>C++ Inference Engine:</strong> Complete implementation with ONNX Runtime 1.19.0</li>
  <li>‚úÖ <strong>Tokenizer Integration:</strong> HuggingFace tokenizers working via C++ bindings</li>
  <li>‚úÖ <strong>GPU Acceleration:</strong> CUDA execution provider with cuDNN 9</li>
  <li>‚úÖ <strong>KV Cache Management:</strong> Efficient cache handling across iterations</li>
  <li>‚úÖ <strong>End-to-End Pipeline:</strong> Input text ‚Üí tokens ‚Üí inference ‚Üí detokenization ‚Üí output</li>
</ul>
</div>

<h3>Example Output</h3>

<p>The system successfully generates coherent text responses (In Progress):</p>

<!-- <div class="codehilite"><pre><span></span><code><span class="gp">$ </span>./build/llm_inference
<span class="go">Loading model: model/qwen_model.onnx</span>
<span class="go">Initializing CUDA execution provider...</span>
<span class="go">Model loaded successfully!</span>

<span class="gp">User: </span>What is the capital of France?
<span class="go">Assistant: The capital of France is Paris. It is located in the north-central</span>
<span class="go">part of the country and is known for its iconic landmarks such as the Eiffel</span>
<span class="go">Tower, the Louvre Museum, and Notre-Dame Cathedral.</span> -->
</code></pre></div>

<h3>Performance Benchmarking (In Progress)</h3>

<div class="highlight-box highlight-warning">
<p><strong>Note:</strong> Comprehensive performance benchmarking and token generation speed calculations are currently <strong>in progress</strong>. While the system is functionally complete and producing correct outputs, I'm still working on:</p>
<ul>
  <li>üìä Accurate tokens/second measurements across different input lengths</li>
  <li>üìä Latency profiling (prefill vs decode phases)</li>
  <li>üìä Memory usage analysis and optimization</li>
  <li>üìä Comparison with PyTorch and Python ONNX Runtime baselines</li>
  <li>üìä Benchmarks across different GPU configurations</li>
</ul>
<p>Initial informal testing shows the system running smoothly on GPU, but formal metrics will be added once the benchmarking suite is complete.</p>
</div>

<!-- <p>The remaining work is quantifying performance and optimization, not making it work in the first place.</p> -->

<div class="highlight-box highlight-info">
<p><strong>Try it yourself:</strong> The complete code is available in the <a href="https://github.com/sgowdaks/llm-inference">GitHub repository</a>. Follow the README for build instructions and running your own inference.</p>
</div>


<!-- <h2>Resources</h2>
<ul>
<li><strong>GitHub Repository:</strong> <a href="https://github.com/sgowdaks/llm-inference">github.com/sgowdaks/llm-inference</a></li>
<li><strong>Native-LLM-for-Android:</strong> <a href="https://github.com/DakeQQ/Native-LLM-for-Android">github.com/DakeQQ/Native-LLM-for-Android</a> (Original ONNX export approach)</li>
<li><strong>ONNX Runtime Docs:</strong> <a href="https://onnxruntime.ai/docs/">onnxruntime.ai/docs</a></li>
<li><strong>Qwen Models:</strong> <a href="https://huggingface.co/Qwen">huggingface.co/Qwen</a></li>
<li><strong>ONNX Spec:</strong> <a href="https://github.com/onnx/onnx">github.com/onnx/onnx</a></li>
</ul>
 -->

    </div>

    <div class="container">
      <div class="post-footer-nav">
        <a href="kv-caching-llm-inference-optimization.html" class="nav-btn nav-btn-prev">
          <i class="fas fa-arrow-left"></i>
          <div class="nav-btn-text">
            <span class="nav-label">Previous Post</span>
            <span class="nav-title">KV Caching: The Hidden Trick Behind Fast LLM Inference</span>
          </div>
        </a>
        <a href="positional-embeddings-transformers.html" class="nav-btn nav-btn-next">
          <div class="nav-btn-text">
            <span class="nav-label">Next Post</span>
            <span class="nav-title">Positional Embeddings in Transformers</span>
          </div>
          <i class="fas fa-arrow-right"></i>
        </a>
      </div>

      <div class="post-tags">
        <span class="tag">LLM</span>
        <span class="tag">ONNX</span>
        <span class="tag">C++</span>
        <span class="tag">GPU</span>
        <span class="tag">Performance</span>
        <span class="tag">AI</span>
      </div>
    </div>

  </div>

  <!-- Footer (auto-generated by common.js) -->
  <footer></footer>

  <!-- Scripts -->
  <script src="../js/common.js"></script>
  
  <!-- Table of Contents Auto-Generator -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      console.log('TOC Script loaded');
      const tocList = document.getElementById('toc-list');
      const postContent = document.querySelector('.post-content');
      const headings = postContent.querySelectorAll('h2');
      
      console.log('Found ' + headings.length + ' headings');
      
      // Generate TOC items
      headings.forEach((heading, index) => {
        // Create ID for heading if it doesn't have one
        if (!heading.id) {
          heading.id = 'section-' + index;
        }
        
        // Create TOC list item
        const li = document.createElement('li');
        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;
        a.addEventListener('click', function(e) {
          e.preventDefault();
          heading.scrollIntoView({ behavior: 'smooth', block: 'start' });
          
          // Update active state
          document.querySelectorAll('.toc-list a').forEach(link => link.classList.remove('active'));
          a.classList.add('active');
        });
        
        li.appendChild(a);
        tocList.appendChild(li);
      });
      
      console.log('TOC generated successfully');
      
      // Highlight active section on scroll
      window.addEventListener('scroll', function() {
        let current = '';
        
        headings.forEach(heading => {
          const sectionTop = heading.offsetTop;
          const scrollPos = window.scrollY + 150;
          
          if (scrollPos >= sectionTop) {
            current = heading.id;
          }
        });
        
        document.querySelectorAll('.toc-list a').forEach(a => {
          a.classList.remove('active');
          if (a.getAttribute('href') === '#' + current) {
            a.classList.add('active');
          }
        });
      });
    });
  </script>
</body>
</html>
